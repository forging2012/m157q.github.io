<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0"><channel><title>Just for noting</title><link>https://blog.m157q.tw/</link><description></description><lastBuildDate>Fri, 04 Aug 2017 21:37:05 +0800</lastBuildDate><item><title>GCPUG.tw #28</title><link>https://blog.m157q.tw/posts/2017/08/04/gcpug-tw-28/</link><description>&lt;ul&gt;
&lt;li&gt;Links  &lt;ul&gt;
&lt;li&gt;&lt;a href="https://gcpugtw.kktix.cc/events/meetup201708"&gt;https://gcpugtw.kktix.cc/events/meetup201708&lt;/a&gt;  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;hr /&gt;
&lt;h1&gt;Running Workloads in Kubernetes - Janet Kuo&lt;/h1&gt;
&lt;p&gt;今年 2/28 時，&lt;a href="https://techcrunch.com/2017/02/28/amazon-aws-s3-outage-is-breaking-things-for-a-lot-of-websites-and-apps/"&gt;Amazon S3 的 outage&lt;/a&gt; 讓很多公司的網站都掛了，&lt;br /&gt;
但有間叫 &lt;a href="https://twitter.com/robertjscott/status/836743514423713793"&gt;Spire 的公司沒受到影響&lt;/a&gt;，&lt;br /&gt;
原因是因為用了 Kubernetes。  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Demo code  &lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/janetkuo/k8s-demos"&gt;https://github.com/janetkuo/k8s-demos&lt;/a&gt;  &lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/kubernetes/contrib/tree/master/micro-demos"&gt;https://github.com/kubernetes/contrib/tree/master/micro-demos&lt;/a&gt;  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Architecture of Kubernetes  &lt;ul&gt;
&lt;li&gt;Master node  &lt;ul&gt;
&lt;li&gt;Controller  &lt;ul&gt;
&lt;li&gt;管理 Worker nodes 上面的 Pod  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Worker node  &lt;ul&gt;
&lt;li&gt;Running Pods  &lt;/li&gt;
&lt;li&gt;Pull image  &lt;/li&gt;
&lt;li&gt;把 Network, Volume 連接起來  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Four General Patterns  &lt;ul&gt;
&lt;li&gt;Stateless  &lt;ul&gt;
&lt;li&gt;Web frontends  &lt;/li&gt;
&lt;li&gt;Web servers  &lt;ul&gt;
&lt;li&gt;NGINX  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Stateful  &lt;ul&gt;
&lt;li&gt;Databases  &lt;ul&gt;
&lt;li&gt;MongoDB  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Message queues  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Daemon  &lt;ul&gt;
&lt;li&gt;Cluster storage  &lt;/li&gt;
&lt;li&gt;Logs collections  &lt;/li&gt;
&lt;li&gt;Node monitoring  &lt;/li&gt;
&lt;li&gt;example  &lt;ul&gt;
&lt;li&gt;linkerd  &lt;/li&gt;
&lt;li&gt;fluentd  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Batch  &lt;ul&gt;
&lt;li&gt;Emails to send  &lt;/li&gt;
&lt;li&gt;Files to zip  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Deployment (For stateless pattern)  &lt;ul&gt;
&lt;li&gt;No persistent states  &lt;ul&gt;
&lt;li&gt;Scale is easy  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Availability &amp;gt; Consistency  &lt;ul&gt;
&lt;li&gt;Create multiple replicas of the smae pod  &lt;/li&gt;
&lt;li&gt;Pods are disposable  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Rolling update  &lt;ul&gt;
&lt;li&gt;Update at a controlled rate  &lt;/li&gt;
&lt;li&gt;Block updates on failure  &lt;/li&gt;
&lt;li&gt;History and rollback  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;用 &lt;code&gt;kubectl&lt;/code&gt; 下指令，會對 Master node 下一個 async 的指令，Master Node 再透過 controller 去控制 Worker node  &lt;/li&gt;
&lt;li&gt;Service  &lt;ul&gt;
&lt;li&gt;去跟後面的 Pods 溝通  &lt;/li&gt;
&lt;li&gt;當有 Pod 掛掉的話，Service 的 load balancer 就不會把 request 導到壞掉的 Pod  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Demo code  &lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/janetkuo/k8s-demos/tree/master/dep"&gt;https://github.com/janetkuo/k8s-demos/tree/master/dep&lt;/a&gt;  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;StatefulSet (For stateful pattern, 另外一個專門針對 Stateful 使用的 controller)  &lt;ul&gt;
&lt;li&gt;可能的使用情境  &lt;ul&gt;
&lt;li&gt;ZooKeeper  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Store persistent data  &lt;ul&gt;
&lt;li&gt;Need stable, unique and sticky identity and storage  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Consistency &amp;gt; Availability  &lt;ul&gt;
&lt;li&gt;Create similar pods, each has its own identity and storage  &lt;/li&gt;
&lt;li&gt;Pods are not disposable  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Deploy, scale, terminate  &lt;ul&gt;
&lt;li&gt;In order or in parallel  &lt;/li&gt;
&lt;li&gt;可以設定依序 deploy，因為有些服務不能全部一起開，而是必須照順序開。但如果沒有這個需求的話，還是可以用 Parallel。  &lt;/li&gt;
&lt;li&gt;StatefulSet 的 Parallel Deployment 是在 k8s 1.7 加入的功能  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Rolling update  &lt;ul&gt;
&lt;li&gt;StatefulSet 的 parallel rolling update 是在 k8s 1.7 加入的功能  &lt;/li&gt;
&lt;li&gt;會有 graceful 的 termination，預設的 timeout 時間是 30 秒，如果超過的話，termination 就會被取消。  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Demo  &lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/janetkuo/k8s-demos/tree/master/stateful"&gt;https://github.com/janetkuo/k8s-demos/tree/master/stateful&lt;/a&gt;  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;DaemonSet (For Daemon Pattern)  &lt;ul&gt;
&lt;li&gt;One Daemon per node  &lt;ul&gt;
&lt;li&gt;Background process  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Daemons created and removed with nodes  &lt;/li&gt;
&lt;li&gt;Node labels  &lt;ul&gt;
&lt;li&gt;Control which nodes daemons should run on  &lt;/li&gt;
&lt;li&gt;可以用來控制讓 daemon 只執行在特定的 node 上  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Rolling update  &lt;ul&gt;
&lt;li&gt;在 k8s 1.6 新增的功能  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Demo  &lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/janetkuo/k8s-demos/tree/master/ds"&gt;https://github.com/janetkuo/k8s-demos/tree/master/ds&lt;/a&gt;  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Jobs (For Batch Pattern)  &lt;ul&gt;
&lt;li&gt;Run in Parallel  &lt;ul&gt;
&lt;li&gt;How many pods can be created and running at a time  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Run to Completion  &lt;ul&gt;
&lt;li&gt;How many pods need to complete (exit successfully)  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Parallel processing of independent but related work items  &lt;ul&gt;
&lt;li&gt;Emails to send or frames to render  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Job create 出來的 pod 是會結束的，因為事情做完了就會關掉。與前面 3 個 pattern 的 controller 建立的 Pod 不同，如果壞掉了不會幫你重開。  &lt;/li&gt;
&lt;li&gt;Demo code  &lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/janetkuo/k8s-demos/tree/master/jobs"&gt;https://github.com/janetkuo/k8s-demos/tree/master/jobs&lt;/a&gt;  &lt;/li&gt;
&lt;li&gt;parallelism: 一次最多開幾個 pod  &lt;/li&gt;
&lt;li&gt;completions: 需要幾個成功的 pods 才能停止  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;4 Controllers for 4 Patterns  &lt;ul&gt;
&lt;li&gt;Deployment  &lt;ul&gt;
&lt;li&gt;Availability  &lt;/li&gt;
&lt;li&gt;Scale &amp;amp; recover easily  &lt;/li&gt;
&lt;li&gt;Disposable cpoies of the same pod  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;StatefulSet  &lt;ul&gt;
&lt;li&gt;Consistency  &lt;/li&gt;
&lt;li&gt;Unique, sticky identity and storage  &lt;/li&gt;
&lt;li&gt;Deploy, scale and terminate in order or in parallel  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;DaemonSet  &lt;ul&gt;
&lt;li&gt;One pod per node by default  &lt;/li&gt;
&lt;li&gt;Daemon pods added and removed with nodes  &lt;/li&gt;
&lt;li&gt;Use node labels to control  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Jobs  &lt;ul&gt;
&lt;li&gt;Run multiple pods in parallel  &lt;/li&gt;
&lt;li&gt;Run pods to completion  &lt;ul&gt;
&lt;li&gt;只有指定數量的 pods 都成功結束的話，這個 job 才算成功  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Where Do I start?  &lt;ul&gt;
&lt;li&gt;Get a Kubernetes Cluster  &lt;ul&gt;
&lt;li&gt;GCE  &lt;/li&gt;
&lt;li&gt;GKE  &lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/kubernetes/minikube"&gt;minikube&lt;/a&gt;  &lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/kubernetes/charts"&gt;Kubernetes Charts&lt;/a&gt;  &lt;ul&gt;
&lt;li&gt;Curated applications for Kubernetes  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/kubernetes/helm"&gt;Kubernetes Helm&lt;/a&gt;  &lt;ul&gt;
&lt;li&gt;The Kubernetes Package Manager  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;How Do I Customize?  &lt;ul&gt;
&lt;li&gt;Kubernetes is extensible  &lt;ul&gt;
&lt;li&gt;You can write your own controller or use controllers wrote by other people.  &lt;/li&gt;
&lt;li&gt;Examples  &lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/upmc-enterprises/elasticsearch-operator"&gt;https://github.com/upmc-enterprises/elasticsearch-operator&lt;/a&gt;  &lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/coreos/etcd-operator"&gt;https://github.com/coreos/etcd-operator&lt;/a&gt;  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Kubernetes is Open  &lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/kubernetes/kubernetes"&gt;https://github.com/kubernetes/kubernetes&lt;/a&gt;  &lt;/li&gt;
&lt;li&gt;&lt;a href="https://kubernetes.io"&gt;https://kubernetes.io&lt;/a&gt;  &lt;/li&gt;
&lt;li&gt;&lt;a href="https://slack.k8s.io"&gt;https://slack.k8s.io&lt;/a&gt;  &lt;/li&gt;
&lt;li&gt;&lt;a href="https://twitter.com/kubernetesio"&gt;https://twitter.com/kubernetesio&lt;/a&gt;  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Q&amp;amp;A  &lt;ul&gt;
&lt;li&gt;StatefulSet 的 Storage 不會被砍掉，那 DaemonSet 可以有一樣的效果嗎？因為有時候一些 Daemon 會需要這方式。  &lt;ul&gt;
&lt;li&gt;可以，但只能讀，不能寫，因為 DaemonSet 的設計上的關係，如果大家都可以寫的話會很亂。  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Rolling Update 如果希望新舊版都同時存在的話該怎麼做  &lt;ul&gt;
&lt;li&gt;StatefulSet 可以透過 Partition 做到  &lt;/li&gt;
&lt;li&gt;Deployment 的話，可以建立一個新的和一個舊的 Deployment，再透過 Service 的 Load Balancer 去導流量。  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Rolling Update 如果不希望新舊版都同時存在的話該怎麼做  &lt;ul&gt;
&lt;li&gt;StatefulSet 沒辦法做到  &lt;/li&gt;
&lt;li&gt;但 Deployment 可以把 strategy 設定成 Recreate，這樣就會先起好新的 pod，然後把舊的砍掉。  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">m157q</dc:creator><pubDate>Fri, 04 Aug 2017 21:37:05 +0800</pubDate><guid isPermaLink="false">tag:blog.m157q.tw,2017-08-04:posts/2017/08/04/gcpug-tw-28/</guid><category>Google Cloud Platform</category></item><item><title>GCPUG.tw #27</title><link>https://blog.m157q.tw/posts/2017/07/05/gcpug-tw-27/</link><description>&lt;p&gt;Event link: &lt;a href="https://gcpugtw.kktix.cc/events/meetup201707"&gt;https://gcpugtw.kktix.cc/events/meetup201707&lt;/a&gt;  &lt;/p&gt;
&lt;hr /&gt;
&lt;h2&gt;Using Kubernetes to deploy Django in GCP&lt;/h2&gt;
&lt;p&gt;Speaker: Walter Liu&lt;br /&gt;
Slides: &lt;a href="https://www.slideshare.net/walterliu7/using-kubernetes-to-deploy-django-in-gcp"&gt;https://www.slideshare.net/walterliu7/using-kubernetes-to-deploy-django-in-gcp&lt;/a&gt;  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://kubernetes.io/docs/tutorials/stateful-application/basic-stateful-set/"&gt;Statefulset&lt;/a&gt;  &lt;ul&gt;
&lt;li&gt;Beta feature since k8s 1.7  &lt;/li&gt;
&lt;li&gt;Sharding Service  &lt;/li&gt;
&lt;li&gt;還是得設定 service 給他，不然 DNS lookup 會找不到  &lt;/li&gt;
&lt;li&gt;Like deployment with static POD name  &lt;/li&gt;
&lt;li&gt;Usage: Sharded service  &lt;ul&gt;
&lt;li&gt;redis  &lt;/li&gt;
&lt;li&gt;memcached  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Other usage: static volume  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Secret / ConfigMap  &lt;/li&gt;
&lt;li&gt;Service  &lt;/li&gt;
&lt;li&gt;Ingress  &lt;ul&gt;
&lt;li&gt;Global Load Balancer  &lt;/li&gt;
&lt;li&gt;No firewall ability  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;k8s has no crontab (currently)  &lt;ul&gt;
&lt;li&gt;Use Celery  &lt;/li&gt;
&lt;li&gt;Use crontab in Google App Engine  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;k8s + GCP Load Balancer  &lt;/li&gt;
&lt;li&gt;Cluster creation steps  &lt;ul&gt;
&lt;li&gt;&lt;code&gt;kubectl create -f web_secretes.yaml&lt;/code&gt;  &lt;/li&gt;
&lt;li&gt;&lt;code&gt;kubectl apply -f cache_stateful_set.yaml&lt;/code&gt;  &lt;/li&gt;
&lt;li&gt;&lt;code&gt;./titan_control deploy prod&lt;/code&gt;  &lt;ul&gt;
&lt;li&gt;Like =&amp;gt; &lt;code&gt;kubectl apply -f prod_web_deploy.yaml&lt;/code&gt;  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;code&gt;kubectl apply -f service.yaml&lt;/code&gt;  &lt;/li&gt;
&lt;li&gt;&lt;code&gt;kubectl apply -f ingress.yaml&lt;/code&gt;  &lt;/li&gt;
&lt;li&gt;&lt;code&gt;gsutil mb -l asia gs://static.example.com&lt;/code&gt;  &lt;/li&gt;
&lt;li&gt;要記得檢查該開啟來的 service 有沒有開起來  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;遇到的問題  &lt;ul&gt;
&lt;li&gt;Templating  &lt;ul&gt;
&lt;li&gt;Use Python Jinja to do k8s templating  &lt;/li&gt;
&lt;li&gt;Someone had suggested me to use &lt;a href="https://github.com/kubernetes/helm"&gt;HELM&lt;/a&gt;  &lt;/li&gt;
&lt;li&gt;Show templating example  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;hr /&gt;
&lt;h2&gt;深入 Kubernetes Network，Calico Overlet Network 介紹&lt;/h2&gt;
&lt;p&gt;Speaker: 光光&lt;br /&gt;
Slides: &lt;a href="https://www.slideshare.net/IsaacTseng/20170705-kubernetes-with-calico"&gt;https://www.slideshare.net/IsaacTseng/20170705-kubernetes-with-calico&lt;/a&gt;  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;k8s cluster network  &lt;ul&gt;
&lt;li&gt;inside a pod  &lt;ul&gt;
&lt;li&gt;App &amp;amp; DB connection with local network  &lt;ul&gt;
&lt;li&gt;因為在同個 Pod 走內網，所以不會有 performance 的問題  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;pod-to-service  &lt;ul&gt;
&lt;li&gt;Conneciton via Service  &lt;ul&gt;
&lt;li&gt;不會有啥太大的問題  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;external-to-service  &lt;ul&gt;
&lt;li&gt;Conneciton via Service  &lt;ul&gt;
&lt;li&gt;不會有啥太大的問題  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;pod-to-pod  &lt;ul&gt;
&lt;li&gt;Pod &amp;amp; Pod may in different hosts  &lt;/li&gt;
&lt;li&gt;How to connect two Pods?  &lt;/li&gt;
&lt;li&gt;這會是今天主要探討的部份  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Overlay network  &lt;ul&gt;
&lt;li&gt;Flannel  &lt;ul&gt;
&lt;li&gt;CoreOS 使用 &lt;a href="https://github.com/coreos/flannel"&gt;Flannel&lt;/a&gt;  &lt;/li&gt;
&lt;li&gt;All pockets go through Flannel  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Performance  &lt;ul&gt;
&lt;li&gt;Overlay vs Underlay  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;User story  &lt;ul&gt;
&lt;li&gt;IPsec Tunneling  &lt;ul&gt;
&lt;li&gt;在內部網路仍然使用 IPsec Tunneling 導致 throughput Performance 下降了約 80%  &lt;/li&gt;
&lt;li&gt;為了想要解決這個問題，所以用上了 Calico  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="http://rancher.com/rancher-os/"&gt;RancherOS&lt;/a&gt;  &lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/rancher/os"&gt;https://github.com/rancher/os&lt;/a&gt;  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/projectcalico/calico"&gt;Calico&lt;/a&gt;  &lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.projectcalico.org//"&gt;https://www.projectcalico.org//&lt;/a&gt;  &lt;/li&gt;
&lt;li&gt;Features  &lt;ul&gt;
&lt;li&gt;Use etcd  &lt;/li&gt;
&lt;li&gt;BGP Routing  &lt;/li&gt;
&lt;li&gt;Pod get routings to other Pod  &lt;/li&gt;
&lt;li&gt;Packets not over Calico  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;How to use  &lt;ul&gt;
&lt;li&gt;Kubelet settings: &lt;code&gt;--network-plugin=cni&lt;/code&gt;  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;CoreOS 的相容性不太高，要做網路層的實體 binding 的話會比較複雜，可能需要多注意一點。  &lt;/li&gt;
&lt;li&gt;&lt;a href="https://kubernetes.io/docs/concepts/workloads/controllers/daemonset/"&gt;Daemon Set&lt;/a&gt;  &lt;/li&gt;
&lt;li&gt;&lt;code&gt;kubectl apply -f calico.yaml&lt;/code&gt;  &lt;/li&gt;
&lt;li&gt;&lt;code&gt;kubectl get pod --namespace=kube-system&lt;/code&gt;  &lt;/li&gt;
&lt;li&gt;為什麼選 Calico 不選 Flannel?  &lt;ul&gt;
&lt;li&gt;因為 Flannel 是 Overlay Network，而且不在 CoreOS 裏面用的話會很麻煩  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;為什麼 GCP 比 Calico 更好？  &lt;ul&gt;
&lt;li&gt;GKE 是執行在 GCE 裏面，然後 Network 是直接跟 GCE 作 bridge，所以 Pod 之間的溝通會很方便。  &lt;/li&gt;
&lt;li&gt;但會有個問題，如果要做一些安全性的限制的話會比較麻煩，因為大家的網路都是 bridge 在一起，所以比較難針對這點去做限制。  &lt;/li&gt;
&lt;li&gt;如果有這方面的考量的話，建議不要直接用 GKE，但可以試著在 GCE 上安裝 Calico 來使用。  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Calico node 會去處理他擁有的 IP，寫在 etcd 裡頭。BGP peering 之後，IP 就可以共享。  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;BGP Peering  &lt;ul&gt;
&lt;li&gt;可以透過 BGP Peering 的方式，把 Pod 之間相連起來。  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;有興趣的人也可以去玩一下 GCP 上的 &lt;a href="https://cloud.google.com/container-optimized-os/docs/"&gt;Container-Optimized OS&lt;/a&gt;  &lt;/li&gt;
&lt;/ul&gt;
&lt;hr /&gt;
&lt;h2&gt;從 AWS 轉移到 GCP，一個新創團隊搬家的故事: TABLEAPP Architecture Story&lt;/h2&gt;
&lt;p&gt;Speaker: 陳彥文&lt;br /&gt;
Slides: &lt;a href="https://www.slideshare.net/wenchen3/from-aws-to-gcp-tableapp-architecture-story"&gt;https://www.slideshare.net/wenchen3/from-aws-to-gcp-tableapp-architecture-story&lt;/a&gt;  &lt;/p&gt;
&lt;p&gt;幫一家新創公司解決 Server 維護上的問題  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;找出問題  &lt;ul&gt;
&lt;li&gt;展開完整架構  &lt;/li&gt;
&lt;li&gt;列出操遇到的狀況  &lt;/li&gt;
&lt;li&gt;儘可能開啟所有 log  &lt;/li&gt;
&lt;li&gt;分析狀況來源  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;選定改善目標及檢驗標準  &lt;ul&gt;
&lt;li&gt;降低成本  &lt;/li&gt;
&lt;li&gt;資源使用效率  &lt;/li&gt;
&lt;li&gt;減緩月支出成長速度  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Other Benefits of Docker  &lt;ul&gt;
&lt;li&gt;Version Control  &lt;ul&gt;
&lt;li&gt;push code and build  &lt;/li&gt;
&lt;li&gt;every commit has its own images  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Lightweight  &lt;/li&gt;
&lt;li&gt;Isolation  &lt;/li&gt;
&lt;li&gt;Portable  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;How about K8S on EC2?  &lt;ul&gt;
&lt;li&gt;Deploy kubernets on EC2  &lt;ul&gt;
&lt;li&gt;Maintain Kubernetes yourself  &lt;ul&gt;
&lt;li&gt;Install  &lt;/li&gt;
&lt;li&gt;Testing  &lt;/li&gt;
&lt;li&gt;Updating  &lt;/li&gt;
&lt;li&gt;and ...  &lt;/li&gt;
&lt;li&gt;Kubernetes updates really fast  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Handle auto-scaling manually  &lt;ul&gt;
&lt;li&gt;Remove pods before remove ec2  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Using CI to deploy image  &lt;/li&gt;
&lt;li&gt;Set up logging and monitoring policy  &lt;/li&gt;
&lt;li&gt;Integrate AWS resource and Kubernetes manually  &lt;ul&gt;
&lt;li&gt;It must be scalabel  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;It sucks...  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Kubernetes on GCP is awesome  &lt;ul&gt;
&lt;li&gt;GKE  &lt;ul&gt;
&lt;li&gt;Easy  &lt;/li&gt;
&lt;li&gt;Full managed  &lt;ul&gt;
&lt;li&gt;even update kubernetes  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Logging and Monitoring support (stackdrive)  &lt;/li&gt;
&lt;li&gt;Automatic and configurable cluster scaling  &lt;/li&gt;
&lt;li&gt;Google Cloud Platform resource integration  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Architecture  &lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.slideshare.net/wenchen3/from-aws-to-gcp-tableapp-architecture-story/17"&gt;GCP&lt;/a&gt;  &lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.slideshare.net/wenchen3/from-aws-to-gcp-tableapp-architecture-story/18"&gt;Inside k8s&lt;/a&gt;  &lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.slideshare.net/wenchen3/from-aws-to-gcp-tableapp-architecture-story/19"&gt;Deployment&lt;/a&gt;  &lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.slideshare.net/wenchen3/from-aws-to-gcp-tableapp-architecture-story/20"&gt;Log Collection&lt;/a&gt;  &lt;/li&gt;
&lt;li&gt;
&lt;blockquote&gt;
&lt;p&gt;這架構挺棒的，雖然不大，但把可以用的都用上了，可以學習一下。  &lt;/p&gt;
&lt;/blockquote&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Cost and Usage  &lt;ul&gt;
&lt;li&gt;Hybrid 時期的 cost 有比較高  &lt;/li&gt;
&lt;li&gt;後來做完 migration 到 GCP 之後，cost 比在 AWS 上降一半，然後 handle 的 request 量多 5 倍。  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Conclusion  &lt;ul&gt;
&lt;li&gt;CloudCDN is really fast  &lt;ul&gt;
&lt;li&gt;very low latency  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Lower price  &lt;ul&gt;
&lt;li&gt;based on new arch, we save 40% cost  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;平台終究只是工具，人才是最大的關鍵。  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">m157q</dc:creator><pubDate>Wed, 05 Jul 2017 21:46:56 +0800</pubDate><guid isPermaLink="false">tag:blog.m157q.tw,2017-07-05:posts/2017/07/05/gcpug-tw-27/</guid><category>Google Cloud Platform</category></item><item><title>GCPUG.tw #26</title><link>https://blog.m157q.tw/posts/2017/06/02/gcpug-tw-26/</link><description>&lt;p&gt;Event link: &lt;a href="https://gcpugtw.kktix.cc/events/meetup201706"&gt;https://gcpugtw.kktix.cc/events/meetup201706&lt;/a&gt;  &lt;/p&gt;
&lt;hr /&gt;
&lt;h3&gt;GCP Next 2017 recap&lt;/h3&gt;
&lt;h4&gt;Keynote&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Fe-Fe Li  &lt;/li&gt;
&lt;li&gt;Vint Cerf  &lt;ul&gt;
&lt;li&gt;TCP/IP  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Mac Andreessen  &lt;ul&gt;
&lt;li&gt;Netscape  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Eric Schmidt  &lt;ul&gt;
&lt;li&gt;Google has spent $30B in building Google Cloud in the past 3 years.  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Migrate 到 GCP 的知名企業  &lt;ul&gt;
&lt;li&gt;Disney  &lt;/li&gt;
&lt;li&gt;SAP  &lt;/li&gt;
&lt;li&gt;高露潔  &lt;/li&gt;
&lt;li&gt;Verizon  &lt;/li&gt;
&lt;li&gt;Home Depot  &lt;/li&gt;
&lt;li&gt;HSBC  &lt;ul&gt;
&lt;li&gt;金融業算是對 Cloud 這部份最保守的行業  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;eBay  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;其他重要公佈  &lt;ul&gt;
&lt;li&gt;2017 有 4 個新 Region  &lt;/li&gt;
&lt;li&gt;2018 有 5 個新 Region  &lt;/li&gt;
&lt;li&gt;Cloud Spanner beta  &lt;/li&gt;
&lt;li&gt;Committed-Used Discount  &lt;/li&gt;
&lt;li&gt;Titan  &lt;/li&gt;
&lt;li&gt;Dataprep  &lt;/li&gt;
&lt;li&gt;Video Intelligence API  &lt;ul&gt;
&lt;li&gt;進階版的 Vision API  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Data Loss Prevention (in G Suite)  &lt;/li&gt;
&lt;li&gt;CloudSQL w/ PostgreSQL  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;大會議程&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Kubernetes  &lt;ul&gt;
&lt;li&gt;Fast Develop Productivity  &lt;/li&gt;
&lt;li&gt;Efficient Scale Out  &lt;/li&gt;
&lt;li&gt;Open Architecture  &lt;/li&gt;
&lt;li&gt;Serverless / NoOps  &lt;/li&gt;
&lt;li&gt;Security  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Tensorflow  &lt;ul&gt;
&lt;li&gt;利用 MINST 的手寫字體作為 demo  &lt;/li&gt;
&lt;li&gt;Softmax classification  &lt;/li&gt;
&lt;li&gt;Noisy  &lt;/li&gt;
&lt;li&gt;Overfitting  &lt;/li&gt;
&lt;li&gt;Learning Rate Decay  &lt;/li&gt;
&lt;li&gt;Dropout  &lt;/li&gt;
&lt;li&gt;&lt;a href="https://goo.gl/pHeXe7"&gt;https://goo.gl/pHeXe7&lt;/a&gt;  &lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.comn/martin-gorner/tensorflow-mnist-tutorial"&gt;https://github.comn/martin-gorner/tensorflow-mnist-tutorial&lt;/a&gt;  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Cloud Spanner  &lt;ul&gt;
&lt;li&gt;Google DoubleClick / AdWords 內部自己在用的服務  &lt;/li&gt;
&lt;li&gt;Scalabiltiy - 只要把 instance number 改掉  &lt;/li&gt;
&lt;li&gt;會自己學習你的 Query Pattern, 自動優化資料  &lt;/li&gt;
&lt;li&gt;Interleave Table  &lt;ul&gt;
&lt;li&gt;Table 之間的 Logical relation  &lt;/li&gt;
&lt;li&gt;Pre-computed JOIN  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;也是需要 Warmup  &lt;/li&gt;
&lt;li&gt;有人直接拿來取代 MySQL 來用 (Quizlet)  &lt;ul&gt;
&lt;li&gt;500GB Data, 8 billions rows  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Cloud Storage and Optimization Tuning  &lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;教育訓練與認証&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Codelabs  &lt;ul&gt;
&lt;li&gt;各個 Google Cloud 主題的互動式免費教學  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;GCP 認証  &lt;ul&gt;
&lt;li&gt;Data Engineer  &lt;ul&gt;
&lt;li&gt;設計資料、資料模型  &lt;/li&gt;
&lt;li&gt;什麼資料要用什麼服務放  &lt;ul&gt;
&lt;li&gt;BigTable, BigQuery, Datastore, GCS, ...  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Cloud Architect  &lt;ul&gt;
&lt;li&gt;架構設計、Solution 選擇  &lt;/li&gt;
&lt;li&gt;安全性設計  &lt;/li&gt;
&lt;li&gt;成本最佳化 (Cost-down)  &lt;/li&gt;
&lt;li&gt;現有服務整合  &lt;/li&gt;
&lt;li&gt;要花 200 鎂  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;技術展示&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Jupiter Switch  &lt;ul&gt;
&lt;li&gt;Google 自製的 Data Center Switch  &lt;/li&gt;
&lt;li&gt;單櫃有 1300 Tbps 的 throughput  &lt;/li&gt;
&lt;li&gt;Juniper 最大台的 MX2020 也只有 80 Tbps 的 throughput  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Pokemon Go  &lt;ul&gt;
&lt;li&gt;CRE (Customer Reliability Engineer)  &lt;ul&gt;
&lt;li&gt;專案夠大的話，Google 會派工程師去協助該公司解決問題  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Datastore 無痛 auto-scaling 50 倍的預期流量  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Machine Learning + 農業  &lt;ul&gt;
&lt;li&gt;日本人用 Raspberry Pi + Tensorflow 替農夫用電腦來做小黃瓜等級篩選  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Evernote 搬家  &lt;ul&gt;
&lt;li&gt;在 70 天內搬了 3.5 PB  &lt;/li&gt;
&lt;li&gt;提升使用者存取速度  &lt;/li&gt;
&lt;li&gt;安全性提升  &lt;/li&gt;
&lt;li&gt;搬家的主要原因是為了做 Machine Learning  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;LUSH  &lt;ul&gt;
&lt;li&gt;22 天就把全部環境包含軟體開發流程搬上 GCP  &lt;/li&gt;
&lt;li&gt;典型的先用 G Suite 再考慮 GCP 的客戶  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Cloud Spanner  &lt;ul&gt;
&lt;li&gt;主打 ACID、水平擴展、高可靠度  &lt;/li&gt;
&lt;li&gt;...  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;G Suite  &lt;ul&gt;
&lt;li&gt;Hangout 改版，主打多人會議功能  &lt;/li&gt;
&lt;li&gt;可以 25 個人同時視訊會議  &lt;/li&gt;
&lt;li&gt;Data Loss Prevention  &lt;/li&gt;
&lt;li&gt;Jamboard  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;hr /&gt;
&lt;h3&gt;TensorFlow in the Wild&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Speaker: &lt;a href="https://github.com/kazunori279"&gt;Kaz Sato&lt;/a&gt;  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Staff Developer Advocate Data &amp;amp; Analytics, Google Cloud  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Machine Learning? AI? Neural Network?  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Artificial Intelligence  &lt;ul&gt;
&lt;li&gt;The science of making things smart  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Machine Learning  &lt;ul&gt;
&lt;li&gt;Building machines that can learn  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Neural Network  &lt;ul&gt;
&lt;li&gt;A type of algorithm in machine learning  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Neural Network  &lt;ul&gt;
&lt;li&gt;is a function that can learn  &lt;/li&gt;
&lt;li&gt;tons of multiply and add  &lt;/li&gt;
&lt;li&gt;&lt;a href="http://playground.tensorflow.org"&gt;http://playground.tensorflow.org&lt;/a&gt;  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;ML at Google  &lt;ul&gt;
&lt;li&gt;Almost every products in Google use Maching Learning  &lt;/li&gt;
&lt;li&gt;&lt;a href="https://deepmind.com/blog/wavenet-generative-model-raw-audio"&gt;https://deepmind.com/blog/wavenet-generative-model-raw-audio&lt;/a&gt;  &lt;ul&gt;
&lt;li&gt;Generate bit by bit with Neural Network  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Deteciton of Diabetic disease  &lt;/li&gt;
&lt;li&gt;&lt;a href="https://keynote-video-demo.appspot.com/video/barcelona-webm"&gt;https://keynote-video-demo.appspot.com/video/barcelona-webm&lt;/a&gt; (Login Required)  &lt;ul&gt;
&lt;li&gt;可以分析影片中每一秒出現的物件是什麼，有點像是把每一個 frame 都丟去給 Vision API 後得到的結果  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;Tensorflow&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Open Source  &lt;/li&gt;
&lt;li&gt;Easy to use  &lt;/li&gt;
&lt;li&gt;You can use it to train your own data  &lt;/li&gt;
&lt;li&gt;Protable and Scalable  &lt;ul&gt;
&lt;li&gt;Training on  &lt;ul&gt;
&lt;li&gt;Mac / Windows  &lt;/li&gt;
&lt;li&gt;GPU Server  &lt;/li&gt;
&lt;li&gt;GPU cluster / cloud  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Prediction on  &lt;ul&gt;
&lt;li&gt;Android and iOS  &lt;/li&gt;
&lt;li&gt;RPi and TPU  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;With great tools like &lt;a href="https://www.tensorflow.org/get_started/summaries_and_tensorboard"&gt;TensorBoard&lt;/a&gt;  &lt;/li&gt;
&lt;li&gt;TensorFlow 1.0 released in Feb 2017  &lt;/li&gt;
&lt;li&gt;&lt;a href="https://keras.io/"&gt;Keras&lt;/a&gt;  &lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/kazunori279/TensorFlow-for-absolute-beginners/blob/master/2.%20Classify%20Manhattan%20with%20TensorFlow.ipynb"&gt;Classifying Manhattan with TensorFlow and BigQuery&lt;/a&gt;  &lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/kazunori279/TensorFlow-for-absolute-beginners"&gt;https://github.com/kazunori279/TensorFlow-for-absolute-beginners&lt;/a&gt;  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Community and Eco-System  &lt;/li&gt;
&lt;li&gt;Demo of TensorFlow  &lt;ul&gt;
&lt;li&gt;&lt;a href="https://cloud.google.com/blog/big-data/2016/08/how-a-japanese-cucumber-farmer-is-using-deep-learning-and-tensorflow"&gt;TensorFlow powered Cucumber Sorter&lt;/a&gt;  &lt;ul&gt;
&lt;li&gt;電腦也碼ㄟ選小黃瓜  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.rt-net.jp/karaage1/"&gt;TensorFlow powered Fried Chicken Nugget Server&lt;/a&gt;  &lt;ul&gt;
&lt;li&gt;人工智慧炸雞塊...  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="http://memo.sugyan.com/entry/2016/10/12/084751"&gt;TV popstar face generator with DCGAN&lt;/a&gt;  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;TensorFLow in enterprise  &lt;ul&gt;
&lt;li&gt;The Challenge: Computing Power  &lt;/li&gt;
&lt;li&gt;Cloud Maching Learning Engine (ML Engine)  &lt;ul&gt;
&lt;li&gt;Fully manged distributed training and prediction  &lt;/li&gt;
&lt;li&gt;Scales to tens of CPUs and GPUs  &lt;/li&gt;
&lt;li&gt;Supports custom TensorFlow Graphs  &lt;/li&gt;
&lt;li&gt;HyperTune  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.techrepublic.com/article/7-companies-that-used-machine-learning-to-solve-real-business-problems/"&gt;Real cases&lt;/a&gt;  &lt;ul&gt;
&lt;li&gt;Kewpie: Finding the bad potato cubes (by TensorFlow)  &lt;ul&gt;
&lt;li&gt;A major food manufacturer in Japan  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;AXA: Finding "large loss" car accidents  &lt;ul&gt;
&lt;li&gt;TensorFlow gives 78% accuracy  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;AUCNET IBS  &lt;ul&gt;
&lt;li&gt;&lt;a href="https://konpeki.io/"&gt;https://konpeki.io/&lt;/a&gt;  &lt;/li&gt;
&lt;li&gt;
&lt;blockquote&gt;
&lt;p&gt;AUCNET IBS is a car auction service in Japan. The company relies on multiple photos for each vehicle, and they were previously sorted and categorized manually. AUCNET IBS built an image classifier that detects the model of the car and the parts featured in the photo with 95% accuracy.  &lt;/p&gt;
&lt;/blockquote&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;TensorFlow + BigQuery  &lt;ul&gt;
&lt;li&gt;Define an UDF to calc similariry  &lt;ul&gt;
&lt;li&gt;&lt;a href="https://cloud.google.com/bigquery/user-defined-functions"&gt;UDF: User-defined Functions&lt;/a&gt;  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;hr /&gt;
&lt;h3&gt;Google Genomics API 初探&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Resources  &lt;ul&gt;
&lt;li&gt;&lt;a href="https://cloud.google.com/genomics/"&gt;https://cloud.google.com/genomics/&lt;/a&gt;  &lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/googlegenomics"&gt;https://github.com/googlegenomics&lt;/a&gt;  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;相關知識介紹&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Analogy between biology and computer science  &lt;ul&gt;
&lt;li&gt;Biology vs Python  &lt;ul&gt;
&lt;li&gt;Cell == Computer  &lt;/li&gt;
&lt;li&gt;DNA == *.py source files  &lt;/li&gt;
&lt;li&gt;Genome == All source files  &lt;/li&gt;
&lt;li&gt;RNAs == binaries  &lt;/li&gt;
&lt;li&gt;Proteins == Objects  &lt;/li&gt;
&lt;li&gt;CRISPR == Sed (i.e. &lt;code&gt;s/'ATG'//g+&lt;/code&gt;)  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Genetic Variation  &lt;ul&gt;
&lt;li&gt;每個人都是獨一無二的  &lt;/li&gt;
&lt;li&gt;雙胞胎之間仍然有差異  &lt;/li&gt;
&lt;li&gt;體質  &lt;ul&gt;
&lt;li&gt;吃不胖  &lt;/li&gt;
&lt;li&gt;容易得癌症  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;4 種核甘酸，20 種胺基酸，RNA encoding 每 3 個對應到 1 個胺基酸  &lt;/li&gt;
&lt;li&gt;Mis-Sense Mutation  &lt;/li&gt;
&lt;li&gt;Frame-shift mutation  &lt;ul&gt;
&lt;li&gt;有一個核甘酸突然不見了  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Genetic variant of ALDH2 makes us red face  &lt;ul&gt;
&lt;li&gt;&lt;a href="https://en.wikipedia.org/wiki/Alcohol_flush_reaction"&gt;https://en.wikipedia.org/wiki/Alcohol_flush_reaction&lt;/a&gt;  &lt;/li&gt;
&lt;li&gt;乙醇無法順利代謝成乙醛  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Some drugs works for parts of people  &lt;ul&gt;
&lt;li&gt;有些人吃藥沒效  &lt;/li&gt;
&lt;li&gt;有些人吃藥會過敏  &lt;ul&gt;
&lt;li&gt;&lt;a href="http://www.mayoclinic.org/diseases-conditions/stevens-johnson-syndrome/home/ovc-20317097"&gt;Stevens-Johnson syndrome&lt;/a&gt;  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Precision Medicine  &lt;ul&gt;
&lt;li&gt;&lt;a href="https://en.wikipedia.org/wiki/Precision_medicine"&gt;https://en.wikipedia.org/wiki/Precision_medicine&lt;/a&gt;  &lt;/li&gt;
&lt;li&gt;歐巴馬政府提出  &lt;/li&gt;
&lt;li&gt;透過個人的基因定序，完全瞭解差異以後，針對個人的基因做的個人化醫療服務  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;NGS  &lt;ul&gt;
&lt;li&gt;基因定序的成本在 2007 年左右突然驟降，自此有將其作為區分點，將之後的定序稱為次世代基因定序  &lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.genome.gov/sequencingcosts/"&gt;https://www.genome.gov/sequencingcosts/&lt;/a&gt;  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Next-Generation DNA Sequencing  &lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.ebi.ac.uk/training/online/course/ebi-next-generation-sequencing-practical-course/what-you-will-learn/what-next-generation-dna-"&gt;https://www.ebi.ac.uk/training/online/course/ebi-next-generation-sequencing-practical-course/what-you-will-learn/what-next-generation-dna-&lt;/a&gt;  &lt;/li&gt;
&lt;li&gt;一人份的基因定序 raw data 平均在 200 GB 左右  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;Google Cloud Platform &amp;amp; Google Genomics&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;實例  &lt;ul&gt;
&lt;li&gt;MSSNG Project (AUTISM)  &lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.mss.ng/"&gt;https://www.mss.ng/&lt;/a&gt;  &lt;/li&gt;
&lt;li&gt;和自閉症患者相關的協會合作，收集了大量的自閉症患者的基因  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Million Veteran Project  &lt;/li&gt;
&lt;li&gt;Cancer Investigation  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;How Google Genomics Works  &lt;ul&gt;
&lt;li&gt;&lt;a href="https://cloud.google.com/genomics/overview"&gt;https://cloud.google.com/genomics/overview&lt;/a&gt;  &lt;/li&gt;
&lt;li&gt;Store: Google Cloud Storage  &lt;/li&gt;
&lt;li&gt;Process: Google Genomics  &lt;/li&gt;
&lt;li&gt;Explore: Google BigQuery  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;How to use Google Genomics API?  &lt;ul&gt;
&lt;li&gt;Requirement  &lt;ul&gt;
&lt;li&gt;BigQuery  &lt;/li&gt;
&lt;li&gt;Genomics API  &lt;/li&gt;
&lt;li&gt;Cloud Storage  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Start  &lt;ul&gt;
&lt;li&gt;&lt;a href="https://cloud.google.com/genomics/v1/load-variants"&gt;https://cloud.google.com/genomics/v1/load-variants&lt;/a&gt;  &lt;/li&gt;
&lt;li&gt;Create Dataset (To get  &lt;ul&gt;
&lt;li&gt;&lt;code&gt;gcloud alpha genomics datasets create --name my-dataset&lt;/code&gt;  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Create Variantsets  &lt;/li&gt;
&lt;li&gt;Import Variants  &lt;ul&gt;
&lt;li&gt;&lt;code&gt;gcloud alpha genomics variants import --variantest-id variantset-id&lt;/code&gt;  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Check Operation Details  &lt;ul&gt;
&lt;li&gt;用 GCP 提供的工具看是不是完成了  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Pipelines  &lt;ul&gt;
&lt;li&gt;&lt;a href="https://cloud.google.com/genomics/v1alpha2/pipelines"&gt;https://cloud.google.com/genomics/v1alpha2/pipelines&lt;/a&gt;  &lt;/li&gt;
&lt;li&gt;看了 tutorial 之後，不一定就能套用到自己的 pipeline 上  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="https://googlegenomics.readthedocs.io/en/latest/"&gt;https://googlegenomics.readthedocs.io/en/latest/&lt;/a&gt;  &lt;/li&gt;
&lt;li&gt;Workflow Languages  &lt;ul&gt;
&lt;li&gt;CWL (Common Workflow Language)  &lt;/li&gt;
&lt;li&gt;WDL (Workflow Description Language)  &lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/broadinstitute/wdl"&gt;https://github.com/broadinstitute/wdl&lt;/a&gt;  &lt;/li&gt;
&lt;li&gt;&lt;a href="https://software.broadinstitute.org/wdl/"&gt;https://software.broadinstitute.org/wdl/&lt;/a&gt;  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Others (Makefile, Snakemake, Nextflow, ...)  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Using Cloud ML  &lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/googlegenomics/cloudml-examples"&gt;https://github.com/googlegenomics/cloudml-examples&lt;/a&gt;  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;討論生物資訊的社群  &lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.facebook.com/groups/446434039038963/?ref=br_rs"&gt;Taipei Bioinformatics Omnibus (北-Bio)&lt;/a&gt;  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">m157q</dc:creator><pubDate>Fri, 02 Jun 2017 21:39:53 +0800</pubDate><guid isPermaLink="false">tag:blog.m157q.tw,2017-06-02:posts/2017/06/02/gcpug-tw-26/</guid><category>Google Cloud Platform</category></item><item><title>CPB200: BigQuery for Data Analysts</title><link>https://blog.m157q.tw/posts/2016/12/15/cpb200-bigquery-for-data-analysts/</link><description>&lt;h1&gt;課程資訊&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://myclass.gcptrain.org/"&gt;http://myclass.gcptrain.org/&lt;/a&gt;  &lt;ul&gt;
&lt;li&gt;azkyY21yCg==  &lt;/li&gt;
&lt;li&gt;&lt;a href="https://sites.google.com/a/google.com/cloud-platform-training/cloud-platform-training/cpb200-bigquery-for-data-analysts"&gt;https://sites.google.com/a/google.com/cloud-platform-training/cloud-platform-training/cpb200-bigquery-for-data-analysts&lt;/a&gt;  &lt;/li&gt;
&lt;li&gt;&lt;a href="/files/cpb200-bigquery-for-data-analysts/CPB200.zip"&gt;slides&lt;/a&gt;  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Google 系列課程  &lt;ul&gt;
&lt;li&gt;CP100: 介紹性質，廣而淺，新手課程。  &lt;/li&gt;
&lt;li&gt;CP200: 開始針對單一技術做比較深的介紹。  &lt;/li&gt;
&lt;li&gt;CP300: 五個全天的課程，上完之後一個月內通過線上考試可以拿到 CP300 證照。  &lt;/li&gt;
&lt;li&gt;&lt;a href="https://developers.google.com/experts/"&gt;GDE: Cloud Developer Expert&lt;/a&gt;  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;hr /&gt;
&lt;h1&gt;Day 1: 2016/12/15&lt;/h1&gt;
&lt;hr /&gt;
&lt;h1&gt;Course Introduction&lt;/h1&gt;
&lt;hr /&gt;
&lt;h1&gt;Module 1: Introducing BigQuery&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;SMACK  &lt;ul&gt;
&lt;li&gt;Spark  &lt;/li&gt;
&lt;li&gt;Mesos  &lt;/li&gt;
&lt;li&gt;Akka  &lt;/li&gt;
&lt;li&gt;Cassandra  &lt;/li&gt;
&lt;li&gt;Kafka  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Big Data Current State&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Real-Time Streaming Data  &lt;/li&gt;
&lt;li&gt;Web analysis data  &lt;/li&gt;
&lt;li&gt;IoT sensor data  &lt;/li&gt;
&lt;li&gt;Fraud detection  &lt;/li&gt;
&lt;li&gt;一台 8-core, 32GB RAM 的 server 在普通情況下，大概撐不到 1000 個 concurrent connection  &lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;On Premises Versus Cloud&lt;/h2&gt;
&lt;p&gt;Premises =&amp;gt; Warehouse  &lt;/p&gt;
&lt;p&gt;公司有機房的聽眾：「申請一台機器最快要一個禮拜才能用，一台機器要撐 3~5 年。」  &lt;/p&gt;
&lt;h2&gt;Google's History of Innovation in Big Data&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Google Big Data Stack 1.0  &lt;ul&gt;
&lt;li&gt;2002: GFS  &lt;/li&gt;
&lt;li&gt;2004: MapReduce  &lt;/li&gt;
&lt;li&gt;2005: Bigtable  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Google Big Data Stack 2.0  &lt;ul&gt;
&lt;li&gt;2006: Dremel  &lt;ul&gt;
&lt;li&gt;Apache Drill  &lt;/li&gt;
&lt;li&gt;Query Engine  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;2010: Colossus  &lt;/li&gt;
&lt;li&gt;2011: Megastore  &lt;/li&gt;
&lt;li&gt;2012: Millwheel  &lt;ul&gt;
&lt;li&gt;處理 Streaming 的資料  &lt;/li&gt;
&lt;li&gt;和 Apache Beam 有關  &lt;/li&gt;
&lt;li&gt;Google Cloud Dataflow is based on Millwheel  &lt;/li&gt;
&lt;li&gt;Dataflow 會幫你預測資料大小，然後去做調整。  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;What is BigQuery&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Fully-managed, analytics data warehouse.  &lt;ul&gt;
&lt;li&gt;Near real-time interactive analysis  &lt;ul&gt;
&lt;li&gt;batch / streaming  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;NoOps - No administration for performance and scale  &lt;ul&gt;
&lt;li&gt;但 OPs 還是得注意每天和每個月的 BigQuery 用量  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Reliable  &lt;/li&gt;
&lt;li&gt;Economical  &lt;/li&gt;
&lt;li&gt;Secure  &lt;ul&gt;
&lt;li&gt;可以透過 ACL 分享 dataset  &lt;/li&gt;
&lt;li&gt;Data is encrypted in transport and at rest  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Auditable  &lt;ul&gt;
&lt;li&gt;Immutable logs  &lt;ul&gt;
&lt;li&gt;除非是共用帳號，不然都找得到誰做了什麼事。  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Scalable  &lt;ul&gt;
&lt;li&gt;Virtually unlimited data storeage and processing power.  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Flexible  &lt;ul&gt;
&lt;li&gt;Streaming ingestion: 100K rows/sec per table for real-time data  &lt;/li&gt;
&lt;li&gt;Data mashup: JOIN across diverse datasets/projects  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Easy to use  &lt;ul&gt;
&lt;li&gt;Requires no indexes, keys, or partitions  &lt;/li&gt;
&lt;li&gt;Familiar SQL interface and intuitive UI  &lt;/li&gt;
&lt;li&gt;Nested and repeated field support for schema flexibility  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;BigQuery Is Not&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Transactional RDBMS  &lt;ul&gt;
&lt;li&gt;BigQuery is not an OLTP system  &lt;/li&gt;
&lt;li&gt;BigQuery 不像 RDBMS 要做正規化，反而是要做反正規化，讓資料愈扁平愈好  &lt;/li&gt;
&lt;li&gt;RDBMS 要求 query 要在數毫秒，但 BigQuery 的 query  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Operational Data Store  &lt;ul&gt;
&lt;li&gt;BigQuery is not geared towards capturing live data and applying updates/deletes as they happen in the system of record  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;On-premises solution or appliance  &lt;ul&gt;
&lt;li&gt;BigQuery is a self-contained, cloud-based solution  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Comparisons&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;OLTP (Online Transaciton Processing)  &lt;/li&gt;
&lt;li&gt;OLAP (Online Analytical Processing)  &lt;ul&gt;
&lt;li&gt;Similar in use cases they support  &lt;/li&gt;
&lt;li&gt;BigQuery allows querying via SQL  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;MapReduce  &lt;ul&gt;
&lt;li&gt;Fundamentally a batch oriented technology  &lt;/li&gt;
&lt;li&gt;Higher latency than BigQuery  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;NoSQL  &lt;ul&gt;
&lt;li&gt;Less scalable than BigQuery  &lt;/li&gt;
&lt;li&gt;Awkward or impossible to query - No query language  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;BigQuery Use Cases&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Games and social media analytics  &lt;/li&gt;
&lt;li&gt;Advertising campaign optimization  &lt;/li&gt;
&lt;li&gt;Sensor data analysis  &lt;/li&gt;
&lt;li&gt;POS-Retail Analytics  &lt;/li&gt;
&lt;li&gt;Web logs, machine logs, infrastructure monitoring  &lt;ul&gt;
&lt;li&gt;Google 利用 Machine Learning 分析 Machine logs 來去針對 server 做最佳的管理  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="https://cloud.google.com/solutions/mobile/mobile-gaming-analysis-telemetry"&gt;Mobile Gaming Analytics&lt;/a&gt;  &lt;/li&gt;
&lt;li&gt;&lt;a href="https://cloud.google.com/customers/zulily/"&gt;Zulily  |  Google Cloud Platform&lt;/a&gt;  &lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Lab 1&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://codelabs.developers.google.com/codelabs/cpb200-free-trial/#0"&gt;Sign Up for the Free Trial and Create a Project&lt;/a&gt;  &lt;/p&gt;
&lt;hr /&gt;
&lt;h1&gt;Module 2: BigQuery Functional Overview&lt;/h1&gt;
&lt;h2&gt;BigQuery Project Structure&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Project  &lt;ul&gt;
&lt;li&gt;Top-level structure  &lt;/li&gt;
&lt;li&gt;Contains users, APIs authentications, billing, data, access control lists (control access to datasets and jobs)  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Dataset  &lt;ul&gt;
&lt;li&gt;Named parent of 1 or more tables  &lt;/li&gt;
&lt;li&gt;一堆 Table 的集合  &lt;/li&gt;
&lt;li&gt;Support Partition Tables  &lt;/li&gt;
&lt;li&gt;如果資料存入以後沒有異動，3個月以後，費用會減半。  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Table  &lt;ul&gt;
&lt;li&gt;Columnar structure that stores data  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Job  &lt;ul&gt;
&lt;li&gt;Controls potentially long-running actions  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Projects&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;可以決定要把哪些 Table 分享給哪些人  &lt;/li&gt;
&lt;li&gt;已經做完的 Job 不用 Clean 掉，因為沒有說執行過多少 Job 就不能執行  &lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Datasets&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Reader  &lt;/li&gt;
&lt;li&gt;Editor  &lt;/li&gt;
&lt;li&gt;Owner  &lt;ul&gt;
&lt;li&gt;可以分享  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Tables&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Collection of columns, rows  &lt;/li&gt;
&lt;li&gt;Have a schema  &lt;/li&gt;
&lt;li&gt;Views are supported  &lt;/li&gt;
&lt;li&gt;Can be external (federated)  &lt;ul&gt;
&lt;li&gt;Google Cloud Storage, Google Drive  &lt;/li&gt;
&lt;li&gt;可以從外部載入資料，例如只想存在自己的 Cloud Storage 的資料  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Jobs&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;每一次執行 query 都會產生一個 Job  &lt;/li&gt;
&lt;li&gt;Job 是 by user 紀錄的  &lt;/li&gt;
&lt;li&gt;以前不能 update, 現在可以 update 了  &lt;ul&gt;
&lt;li&gt;可以直接使用 Job ID 來 update insert 的內容  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;BigQuery Storage&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Traditional RDBMS Storage  &lt;ul&gt;
&lt;li&gt;Row based  &lt;/li&gt;
&lt;li&gt;Record-oriented storage  &lt;/li&gt;
&lt;li&gt;透過 index 和正規化讓資料查詢可以更快速  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;BigQuery Storage  &lt;ul&gt;
&lt;li&gt;Column based  &lt;/li&gt;
&lt;li&gt;Columnar storage  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;BigQuery Managed Storage&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;BigQuery data is stored on persistent disks in distributed storage  &lt;/li&gt;
&lt;li&gt;No indexes, keys, or partitions are required  &lt;/li&gt;
&lt;li&gt;Scales to dozens of petabytes  &lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Storage Engine 2006-2015: ColumnIO&lt;/h3&gt;
&lt;p&gt;Date =&amp;gt; Decompress -&amp;gt; Filter (MapReduce) =&amp;gt; Emit  &lt;/p&gt;
&lt;h3&gt;Storage Engine 2016-Now: Capacitor&lt;/h3&gt;
&lt;p&gt;Data =&amp;gt; Emit =&amp;gt; Dictionary =&amp;gt; Filter =&amp;gt; Lookup  &lt;/p&gt;
&lt;p&gt;ref: &lt;a href="https://cloud.google.com/blog/big-data/2016/04/inside-capacitor-bigquerys-next-generation-columnar-storage-format"&gt;https://cloud.google.com/blog/big-data/2016/04/inside-capacitor-bigquerys-next-generation-columnar-storage-format&lt;/a&gt;  &lt;/p&gt;
&lt;h2&gt;BigQuery Processing&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Borg - Cluster management system  &lt;/li&gt;
&lt;li&gt;Dremel query engine  &lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Query Processing 2006-2015&lt;/h3&gt;
&lt;p&gt;Mixer 0 &amp;lt;= Mixer 1 &amp;lt;= Leaf &amp;lt;= Juniper (Network) =&amp;gt; Distributed Storage (Colossus)  &lt;/p&gt;
&lt;h3&gt;Query Processing 2015-Present&lt;/h3&gt;
&lt;p&gt;Master &amp;lt;= Shard &amp;lt;= Shard &amp;lt;= Shard &amp;lt;= Juniper (Network) =&amp;gt; Distributed Storage (Colossus)  &lt;/p&gt;
&lt;h2&gt;BigQuery Web User Interface&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;要不要用 Cache  &lt;/li&gt;
&lt;li&gt;要不要用 Legacy SQL  &lt;/li&gt;
&lt;li&gt;查詢前記得按一下右邊的 Validator，檢查語法順便看一下總共會 Query 多少資料，以免費用爆表。  &lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;BigQuery CLI&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;可以從 Cloud Shell 或 Cloud SDK 使用 bq  &lt;ul&gt;
&lt;li&gt;&lt;code&gt;bq help&lt;/code&gt;  &lt;/li&gt;
&lt;li&gt;&lt;code&gt;bq ls&lt;/code&gt;  &lt;/li&gt;
&lt;li&gt;&lt;code&gt;bq mk&lt;/code&gt;  &lt;/li&gt;
&lt;li&gt;&lt;code&gt;bq load&lt;/code&gt;  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Lab 2&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://codelabs.developers.google.com/codelabs/cpb200-bigquery-interfaces/#0"&gt;BigQuery Interfaces&lt;/a&gt;  &lt;/p&gt;
&lt;hr /&gt;
&lt;h1&gt;Module 3: BigQuery Fundamentals&lt;/h1&gt;
&lt;h2&gt;BigQuery Schemas&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Consist of 1 or more fields (flat or nested)  &lt;/li&gt;
&lt;li&gt;Define the field name, data type, and mode of columns in the table  &lt;/li&gt;
&lt;li&gt;Fields are strongly typed (explicitly defined)  &lt;/li&gt;
&lt;li&gt;Modes indicate whether field data is REQUIRED, NULLABLE or REPEATED  &lt;ul&gt;
&lt;li&gt;REQUIRED fields must contain non-null data  &lt;/li&gt;
&lt;li&gt;NULLABLE fields allow null values  &lt;/li&gt;
&lt;li&gt;REPEATED fields contain an array of values  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Schema Specification&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Specify schema on command line or in JSON file  &lt;/li&gt;
&lt;li&gt;Schema file must contain single array object with entries that provide these properties:  &lt;ul&gt;
&lt;li&gt;"name": Name of the column  &lt;/li&gt;
&lt;li&gt;"type": Type of data  &lt;/li&gt;
&lt;li&gt;"mode" (optional): REQUIRED, NULLABLE or REPEATED  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Example JSON schema:  &lt;/li&gt;
&lt;/ul&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;[  
    {&amp;quot;name&amp;quot;: &amp;quot;name&amp;quot;, &amp;quot;type&amp;quot;: &amp;quot;string&amp;quot;, &amp;quot;mode&amp;quot;: &amp;quot;required&amp;quot;},  
    {&amp;quot;name&amp;quot;: &amp;quot;gender&amp;quot;, &amp;quot;type&amp;quot;: &amp;quot;string&amp;quot;, &amp;quot;mode&amp;quot;: &amp;quot;nullable&amp;quot;},  
    {&amp;quot;name&amp;quot;: &amp;quot;count&amp;quot;, &amp;quot;type&amp;quot;: &amp;quot;integer&amp;quot;, &amp;quot;mode&amp;quot;: &amp;quot;required&amp;quot;}  
]  
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;Denormalized Data in BigQuery&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;BigQuery uses a denormalized or "flat" schema  &lt;ul&gt;
&lt;li&gt;Flatten normalized tables for super-fast querying  &lt;/li&gt;
&lt;li&gt;No performance penalty for sparse columns or duplicate data  &lt;/li&gt;
&lt;li&gt;Pre-join datasets into homogeneous tables  &lt;/li&gt;
&lt;li&gt;Trade JOINS for column scans  &lt;ul&gt;
&lt;li&gt;Storage is more performant and cheaper than compute resources  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Use nested repeated schema (using JSON or AVRO format) to simulate normalization benefits and limit duplication of data  &lt;ul&gt;
&lt;li&gt;Without nesting, aggregation or JOINs are required  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Denormalization Example&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;A normalized one-to-many relationship between people and cities_lived is denormalized (“flattened”) into 1 table  &lt;ul&gt;
&lt;li&gt;A person may have one to many rows in the table  &lt;/li&gt;
&lt;li&gt;Name, age, and gender may be duplicated  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Using nested repeated schema avoids duplication of data  &lt;/li&gt;
&lt;li&gt;Still allows for a flattened table, which retains high performance  &lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;BigQuery Jobs&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;BigQuery defines and executes jobs for:  &lt;ul&gt;
&lt;li&gt;Query execution  &lt;/li&gt;
&lt;li&gt;Loading, copying, and exporting data  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Jobs are atomic  &lt;/li&gt;
&lt;li&gt;Multiple jobs can run concurrently  &lt;/li&gt;
&lt;li&gt;Completed jobs are listed in the jobs collection  &lt;/li&gt;
&lt;li&gt;Jobs have 4 components:  &lt;ul&gt;
&lt;li&gt;Reference – Job ID  &lt;/li&gt;
&lt;li&gt;Configuration – Job task  &lt;/li&gt;
&lt;li&gt;Status – Job state  &lt;/li&gt;
&lt;li&gt;Statistics – Job statistics  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Job History 是 by user 的  &lt;ul&gt;
&lt;li&gt;要看到整個 project 的 BigQuery Job History 的話，要到 Cloud Logging 那邊選擇 BigQuery 查看  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Destination Tables and Caching&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Query results are stored in a destination table  &lt;/li&gt;
&lt;li&gt;Table is temporary or user-defined  &lt;/li&gt;
&lt;li&gt;Temporary tables are:  &lt;ul&gt;
&lt;li&gt;Defined by the service  &lt;/li&gt;
&lt;li&gt;Used as query cache – You are not billed for storage  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;User-defined tables:  &lt;ul&gt;
&lt;li&gt;Remain persistent  &lt;/li&gt;
&lt;li&gt;Are billed at normal storage rates  &lt;/li&gt;
&lt;li&gt;Target location can be any accessible project/dataset  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Query Caching&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Query results are cached to improve performance  &lt;ul&gt;
&lt;li&gt;Subject to same quota policies as non-cached queries  &lt;/li&gt;
&lt;li&gt;Cache results have a size limit of 128 MB compressed  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;No charge for queries that use cached results  &lt;/li&gt;
&lt;li&gt;Results are cached for approximately 24 hours  &lt;ul&gt;
&lt;li&gt;Lifetime extended when a query returns a cached result  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Cache can be turned off  &lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;在 Web UI 要怎麼知道是 Cached Query? =&amp;gt; 仔細看算 query 時間結果後面的小括號，會標示 cached。  &lt;/p&gt;
&lt;h4&gt;Caveats&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Query caching only works for predictable queries  &lt;ul&gt;
&lt;li&gt;When result set is identical to previous run(s) of a query  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Cache is per-user  &lt;ul&gt;
&lt;li&gt;Not shareable across users  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Cache is invalidated if:  &lt;ul&gt;
&lt;li&gt;Data in underlying table(s) changes  &lt;/li&gt;
&lt;li&gt;Query uses dynamic functions, such as &lt;code&gt;CURRENT_TIMESTAMP()&lt;/code&gt; or &lt;code&gt;NOW()&lt;/code&gt;  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Lab 3&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://codelabs.developers.google.com/codelabs/cpb200-bigquery-components/#0"&gt;BigQuery Components and Jobs&lt;/a&gt;  &lt;/p&gt;
&lt;hr /&gt;
&lt;h1&gt;Module 4: Ingesting, Transforming and Storing Data&lt;/h1&gt;
&lt;h2&gt;Ingesting Data&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Load data into BigQuery using CLI, web UI, or API  &lt;/li&gt;
&lt;li&gt;Load data directly into BigQuery storage using streaming insert or a load job  &lt;/li&gt;
&lt;li&gt;Load jobs support:  &lt;ul&gt;
&lt;li&gt;Google Cloud Storage  &lt;ul&gt;
&lt;li&gt;Standard, Durable Reduced Availability, or Nearline (reduced performance)  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Google Drive (JSON, CSV, AVRO, Sheets (first tab only))  &lt;/li&gt;
&lt;li&gt;CSV, JSON, &lt;a href="https://avro.apache.org/docs/current/"&gt;AVRO&lt;/a&gt; file uploads (slower)  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Can also use partner-provided tools/services  &lt;/li&gt;
&lt;li&gt;Load jobs support four data sources:  &lt;ul&gt;
&lt;li&gt;Objects in Google Cloud Storage  &lt;/li&gt;
&lt;li&gt;Data sent with the job or streaming insert  &lt;/li&gt;
&lt;li&gt;A Google Cloud Datastore backup  &lt;/li&gt;
&lt;li&gt;Data in Google Drive  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Data can be:  &lt;ul&gt;
&lt;li&gt;Loaded into a new table  &lt;/li&gt;
&lt;li&gt;Appended to a table  &lt;/li&gt;
&lt;li&gt;Used to overwrite a table  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;BigQuery can ingest both compressed (GZIP) and uncompressed files  &lt;ul&gt;
&lt;li&gt;Highly parallel load operations allow uncompressed files to load significantly faster than compressed files  &lt;/li&gt;
&lt;li&gt;Uncompressed files are larger  &lt;ul&gt;
&lt;li&gt;Possible bandwidth limitations  &lt;/li&gt;
&lt;li&gt;More costly to store  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Weigh compression options based on use case  &lt;/li&gt;
&lt;li&gt;File size limits  &lt;ul&gt;
&lt;li&gt;CSV  &lt;ul&gt;
&lt;li&gt;Compressed: 4GB  &lt;/li&gt;
&lt;li&gt;Uncompressed  &lt;ul&gt;
&lt;li&gt;With newlines: 4GB  &lt;/li&gt;
&lt;li&gt;Without newlines: 5TB  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;JSON (newline delimited)  &lt;ul&gt;
&lt;li&gt;Compressed: 4 GB  &lt;/li&gt;
&lt;li&gt;Uncompressed: 5 TB  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;AVRO  &lt;ul&gt;
&lt;li&gt;Compressed: Compressed files not supported; compressed data blocks are.  &lt;/li&gt;
&lt;li&gt;Uncompressed: 5 TB (2 MB for the file header)  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;Datastore == BigTable + &lt;a href="http://research.google.com/pubs/pub36971.html"&gt;MegaStore&lt;/a&gt;  &lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2&gt;Appending and Reloading Data&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Use CLI or API to append data to an existing table  &lt;/li&gt;
&lt;li&gt;BigQuery does support deletes now (If unchecked the "use Legacy SQL")  &lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Lab 4.1&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://codelabs.developers.google.com/codelabs/cpb200-loading-data/#0"&gt;Loading Data into BigQuery and Using Federated Queries&lt;/a&gt;  &lt;/p&gt;
&lt;h2&gt;Processing/Transforming Data&lt;/h2&gt;
&lt;h3&gt;Schema Design Considerations&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Denormalized versus relational  &lt;ul&gt;
&lt;li&gt;Denormalized yields better performance with some duplication  &lt;/li&gt;
&lt;li&gt;JOINS on relational data are performant but can be slower than queries on denormalized data  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Flat versus nested and repeated  &lt;/li&gt;
&lt;li&gt;Table partitioning  &lt;ul&gt;
&lt;li&gt;Single table for all data  &lt;/li&gt;
&lt;li&gt;Partition data by some range of values (date) - covered later  &lt;/li&gt;
&lt;li&gt;Table decorators (partition, snapshot) - covered later  &lt;ul&gt;
&lt;li&gt;&lt;code&gt;@&lt;/code&gt;  &lt;/li&gt;
&lt;li&gt;&lt;a href="https://cloud.google.com/bigquery/table-decorators"&gt;https://cloud.google.com/bigquery/table-decorators&lt;/a&gt;  &lt;/li&gt;
&lt;li&gt;&lt;a href="http://blog.gdeltproject.org/using-bigquery-table-decorators-to-lower-query-cost/"&gt;http://blog.gdeltproject.org/using-bigquery-table-decorators-to-lower-query-cost/&lt;/a&gt;  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Data Format Considerations&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;BigQuery supports three data formats: AVRO, JSON, CSV  &lt;/li&gt;
&lt;li&gt;AVRO:  &lt;ul&gt;
&lt;li&gt;Open source format that bundles serialized data and schema in same file  &lt;/li&gt;
&lt;li&gt;Supports flat data and nested/repeated fields  &lt;ul&gt;
&lt;li&gt;Nested/repeated data useful for expressing hierarchical data, reduces duplication when denormalizing data  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Loads faster in BigQuery if data contains embedded newlines  &lt;/li&gt;
&lt;li&gt;Limited to 16 MB block size  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;JSON (newline-delimited):  &lt;ul&gt;
&lt;li&gt;Supports flat data and nested/repeated fields  &lt;ul&gt;
&lt;li&gt;Nested/repeated data useful for expressing hierarchical data, reduces duplication when denormalizing data  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Loads faster in BigQuery if data contains embedded newlines  &lt;/li&gt;
&lt;li&gt;One JSON object, including any nested/repeated fields, must appear on each line  &lt;/li&gt;
&lt;li&gt;Supports UTF-8 encoding  &lt;/li&gt;
&lt;li&gt;Limited to 2 MB row size  &lt;/li&gt;
&lt;li&gt;Example  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;{&amp;quot;name&amp;quot;: &amp;quot;abc&amp;quot;, &amp;quot;gender&amp;quot;: &amp;quot;M&amp;quot;, &amp;quot;age&amp;quot;: 38}  
{&amp;quot;name&amp;quot;: &amp;quot;abc&amp;quot;, &amp;quot;gender&amp;quot;: &amp;quot;M&amp;quot;, &amp;quot;age&amp;quot;: 48}  
&lt;/pre&gt;&lt;/div&gt;


&lt;ul&gt;
&lt;li&gt;CSV:  &lt;ul&gt;
&lt;li&gt;Supports flat data only  &lt;ul&gt;
&lt;li&gt;No nested/repeated data  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Supports UTF-8 encoding and ISO-8859-1 encoding  &lt;ul&gt;
&lt;li&gt;If loading ISO-8859-1 encoded data, specify configuration.load.encoding property when creating load job  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Limited to 2 MB row and cell size  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Storing Data&lt;/h2&gt;
&lt;h3&gt;Long-Term Storage in BigQuery&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Automatic discount for data stored longer than 90 days  &lt;ul&gt;
&lt;li&gt;If table data modified, 90-day counter resets  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;No need to delete or archive old data  &lt;ul&gt;
&lt;li&gt;Equivalent to cost of Cloud Storage Nearline  &lt;/li&gt;
&lt;li&gt;量大的時候似乎還是會比較貴，所以還是有人選擇 archive 到 GCS  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;If preferred, store data in Cloud Storage and load into BigQuery when needed  &lt;ul&gt;
&lt;li&gt;No charge for loading/exporting data  &lt;/li&gt;
&lt;li&gt;Loading from Cloud Storage Nearline reduces performance  &lt;/li&gt;
&lt;li&gt;Data in Cloud Storage available to other services  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;hr /&gt;
&lt;h1&gt;Module 5: Pricing and Quotas&lt;/h1&gt;
&lt;p&gt;要自己把費用算的很精確太難了，&lt;br /&gt;
Google Cloud Platform 有各種不同的計費方式，&lt;br /&gt;
這邊的重點當然還是放在怎麼省錢，&lt;br /&gt;
順便瞭解一下計費方式。  &lt;/p&gt;
&lt;hr /&gt;
&lt;h1&gt;Module 6: Clauses and Functions&lt;/h1&gt;
&lt;h2&gt;UDF Constraints&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Max output data size: 5 MB per input row  &lt;/li&gt;
&lt;li&gt;Max number of UDFs that can run concurrently per user = 3  &lt;/li&gt;
&lt;li&gt;&lt;em&gt;Native code JavaScript functions are not supported&lt;/em&gt;  &lt;/li&gt;
&lt;li&gt;The DOM objects Window, Document and Node, and functions that require them, are unsupported  &lt;/li&gt;
&lt;li&gt;Bitwise operations in JavaScript handle only the most significant 32 bits  &lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;UDF Best Practices&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Use UDF test tool to debug to avoid incurring BigQuery charges  &lt;/li&gt;
&lt;li&gt;Avoid persistent mutable state  &lt;ul&gt;
&lt;li&gt;Do not store or access mutable state in UDF calls  &lt;/li&gt;
&lt;li&gt;Each BQ node has local javascript processing environment that may produce local values  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Use memory efficiently to avoid memory exhaustion on local JavaScript environments  &lt;/li&gt;
&lt;li&gt;Explicitly list SELECT columns instead of SELECT * (not supported)  &lt;/li&gt;
&lt;li&gt;Pre-filter input to limit the size of data on which UDF will run  &lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;BigQuery Recipes&lt;/h2&gt;
&lt;p&gt;投影片裡的範例不知道是不是故意排版的很醜讓人看不懂...&lt;br /&gt;
其實只要照下面把 Query statement 排版一下&lt;br /&gt;
再從最裡面的 Query 看到最外面就滿好懂了&lt;br /&gt;
只是像一些 BigQuery 特有的 function: &lt;code&gt;EVERY()&lt;/code&gt;, &lt;code&gt;SOME()&lt;/code&gt;, &lt;code&gt;LAG()&lt;/code&gt; 之類的&lt;br /&gt;
只好到 &lt;a href="https://cloud.google.com/bigquery/docs/reference/legacy-sql"&gt;https://cloud.google.com/bigquery/docs/reference/legacy-sql&lt;/a&gt; 察看文件了  &lt;/p&gt;
&lt;h3&gt;Pivot&lt;/h3&gt;
&lt;p&gt;Find 100 largest words in Shakespeare’s works and display the number of occurrences of those words in four of Shakespears more popular works.  &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;SELECT  
    word,  
    SUM(IF(corpus = &amp;#39;kinglear&amp;#39;, corpus_total, 0)) AS kinglear,  
    SUM(IF(corpus = &amp;#39;tempest&amp;#39;, corpus_total, 0)) AS tempest,  
    SUM(IF(corpus = &amp;#39;macbeth&amp;#39;, corpus_total, 0)) AS macbeth,  
    SUM(IF(corpus = &amp;#39;hamlet&amp;#39;, corpus_total, 0)) AS hamlet,  
    SUM(corpus_total) AS [total]  
FROM (  
    SELECT  
        word,  
        LENGTH(word) AS word_len,  
        corpus,  
        SUM(word_count) AS corpus_total  
    FROM [publicdata:samples.shakespeare]  
    WHERE LENGTH(word) &amp;gt; 10  
    GROUP BY word, word_len, corpus  
)  
GROUP BY word, word_len  
ORDER BY word_len DESC  
LIMIT 100  
&lt;/pre&gt;&lt;/div&gt;


&lt;h3&gt;Cohort Analysis&lt;/h3&gt;
&lt;p&gt;Counts of Wikipedia contributors who contribute only to pages on Manning brothers vs those who contributed to Manning brothers pages and other pages. Query highlights EVERY and SOME functions.  &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;SELECT  
    peyton_all,  
    peyton_some,  
    eli_all,  
    eli_some,  
    COUNT(1) AS NUM,  
    AVG(edits) AS avg_edits  
FROM (  
    SELECT  
        contributor_id,  
        EVERY(peyton_edit) AS peyton_all,  
        SOME(peyton_edit) AS peyton_some,  
        EVERY(eli_edit) AS eli_all,  
        SOME(eli_edit) AS eli_some,  
        COUNT(1) AS edits  
    FROM (  
        SELECT  
            contributor_id,  
            (LOWER(title) CONTAINS &amp;#39;peyton manning&amp;#39;) AS peyton_edit,  
            (LOWER(title) CONTAINS &amp;#39;eli manning&amp;#39;) AS eli_edit  
        FROM [publicdata:samples.wikipedia]  
    )  
    GROUP BY contributor_id  
    HAVING peyton_all OR peyton_some OR eli_all OR eli_some  
)  
GROUP BY peyton_all, peyton_some, eli_all,eli_some  
ORDER BY peyton_all, peyton_some, eli_all,eli_some  
&lt;/pre&gt;&lt;/div&gt;


&lt;h3&gt;Trailing Averages&lt;/h3&gt;
&lt;p&gt;Demonstrates windowing functions to calculate moving average on number Trailing avg of user activities in github.&lt;br /&gt;
Outermost query combines trailing values into a weighted average paying attention to missing values.  &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;SELECT  
    start_date,  
    ((num_0 + if(num_1 &amp;gt; -1, num_1, num_0) * 0.5 + if(num_2 &amp;gt; -1, num_2, num_0) * 0.25) / 1.75) AS smooth_num  
FROM (  
    SELECT  
        start_date, num_0,  
        LAG(num_0, 1, integer(-1)) OVER (ORDER BY start_date) AS num_1,  
        LAG(num_0, 2, INTEGER(-1)) OVER (ORDER BY start_date) AS num_2  
    FROM (  
        SELECT  
            DATE(created_at) as start_date,  
            INTEGER(COUNT(1)) num_0  
        FROM [publicdata:samples.github_timeline]  
        GROUP BY start_date  
    )  
)  
ORDER BY smooth_num DESC  
&lt;/pre&gt;&lt;/div&gt;


&lt;hr /&gt;
&lt;h1&gt;Day 2: 2016/12/16&lt;/h1&gt;
&lt;hr /&gt;
&lt;h1&gt;Module 7: Nested, Repeated, and Nested Repeated Fields&lt;/h1&gt;
&lt;h2&gt;Nested Field&lt;/h2&gt;
&lt;p&gt;"type": "RECORD"  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;BigQuery supports importing and exporting nested fields in JSON and AVRO files  &lt;/li&gt;
&lt;li&gt;A nested record field adds a named substructure to a row of data  &lt;/li&gt;
&lt;li&gt;Useful as a mechanism to organize related information  &lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Example: JSON Nested Schema and Data&lt;/h3&gt;
&lt;p&gt;Schema  &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;[{&amp;quot;name&amp;quot;: &amp;quot;name&amp;quot;, &amp;quot;type&amp;quot;: &amp;quot;string&amp;quot;, &amp;quot;mode&amp;quot;: &amp;quot;REQUIRED&amp;quot;},  
{&amp;quot;name&amp;quot;: &amp;quot;book&amp;quot;, &amp;quot;type&amp;quot;: &amp;quot;RECORD&amp;quot;, &amp;quot;fields&amp;quot;:  
    [  
    {&amp;quot;name&amp;quot;: &amp;quot;title&amp;quot;, &amp;quot;type&amp;quot;: &amp;quot;string&amp;quot;},  
    {&amp;quot;name&amp;quot;: &amp;quot;ISBN&amp;quot;, &amp;quot;type&amp;quot;: &amp;quot;string&amp;quot;}  
    ]  
}]  
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Data (Newline-delimited JSON)  &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;{&amp;quot;name&amp;quot;: &amp;quot;randolph&amp;quot;, &amp;quot;book&amp;quot;: {&amp;quot;title&amp;quot;: &amp;quot;The Beginning&amp;quot;, &amp;quot;ISBN&amp;quot;: &amp;quot;213423422&amp;quot; } }  
{&amp;quot;name&amp;quot;: &amp;quot;charles&amp;quot;, &amp;quot;book&amp;quot;: {&amp;quot;title&amp;quot;: &amp;quot;Fortunate Few&amp;quot;, &amp;quot;ISBN&amp;quot;: &amp;quot;993032933&amp;quot; } }  
{&amp;quot;name&amp;quot;: &amp;quot;james&amp;quot;, &amp;quot;book&amp;quot;: {&amp;quot;title&amp;quot;: &amp;quot;Homeward Bound&amp;quot;, &amp;quot;ISBN&amp;quot;: &amp;quot;884920039&amp;quot; } }  
&lt;/pre&gt;&lt;/div&gt;


&lt;h3&gt;Querying Nested Fields&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;BigQuery automatically flattens nested fields when querying  &lt;ul&gt;
&lt;li&gt;&lt;code&gt;SELECT * ... results in columns &amp;lt;record_name&amp;gt;_&amp;lt;nested_field_name&amp;gt;&lt;/code&gt;  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;To query a specific nested field:  &lt;ul&gt;
&lt;li&gt;&lt;code&gt;SELECT name, book.title FROM [dataset.table]&lt;/code&gt;  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Lab 7.1&lt;/h2&gt;
&lt;p&gt;CSV 無法表示 nest 結構，所以要上傳有 nested fields 的 data 的話，只能用 JSON 或 AVRO  &lt;/p&gt;
&lt;h2&gt;Repeated Fields&lt;/h2&gt;
&lt;p&gt;"mode": "REPEATED"  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;BigQuery supports importing and exporting repeated fields in JSON and AVRO files  &lt;/li&gt;
&lt;li&gt;A repeated field adds an &lt;em&gt;array&lt;/em&gt; of data inside a single field or record  &lt;/li&gt;
&lt;li&gt;Useful as a mechanism to denormalize a foreign table  &lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Example: JSON Repeated Schema and Data&lt;/h3&gt;
&lt;p&gt;Schema  &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;[  
{&amp;quot;name&amp;quot;: &amp;quot;name&amp;quot;, &amp;quot;type&amp;quot;: &amp;quot;string&amp;quot;, &amp;quot;mode&amp;quot;: &amp;quot;REQUIRED&amp;quot; },  
{&amp;quot;name&amp;quot;: &amp;quot;city&amp;quot;, &amp;quot;type&amp;quot;: &amp;quot;string&amp;quot;, &amp;quot;mode&amp;quot;: &amp;quot;REPEATED&amp;quot; },  
{&amp;quot;name&amp;quot;: &amp;quot;book&amp;quot;, &amp;quot;type&amp;quot;: &amp;quot;string&amp;quot;, &amp;quot;mode&amp;quot;: &amp;quot;REPEATED&amp;quot; }  
]  
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Data  &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;{&amp;quot;name&amp;quot;: &amp;quot;randy&amp;quot;, &amp;quot;city&amp;quot;: [&amp;quot;Tucson&amp;quot;, &amp;quot;Houston&amp;quot;, &amp;quot;Seattle&amp;quot;], &amp;quot;book&amp;quot;: [&amp;quot;The Fudge&amp;quot;]}  
{&amp;quot;name&amp;quot;: &amp;quot;charlie&amp;quot;, &amp;quot;city&amp;quot;: [&amp;quot;Tucson&amp;quot;, &amp;quot;Seattle&amp;quot;, &amp;quot;Redmond&amp;quot;], &amp;quot;book&amp;quot;: []}  
{&amp;quot;name&amp;quot;: &amp;quot;cynthia&amp;quot;, &amp;quot;city&amp;quot;: [&amp;quot;Houston&amp;quot;, &amp;quot;Austin&amp;quot;], &amp;quot;book&amp;quot;: [&amp;quot;The Fudge&amp;quot;, &amp;quot;Outlaws&amp;quot;]}  
&lt;/pre&gt;&lt;/div&gt;


&lt;h3&gt;Querying Repeated Fields&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;A query such as &lt;code&gt;SELECT * ...&lt;/code&gt; produces an error:  &lt;ul&gt;
&lt;li&gt;Cannot output multiple independently repeated fields at the same time  &lt;/li&gt;
&lt;li&gt;一定只能 flatten 其中一個 repeated field (下面 Querying Multiple Repeated Fields 的作法）  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Querying one repeated field, yields automatically flattened result  &lt;ul&gt;
&lt;li&gt;Example: &lt;code&gt;SELECT name, book FROM [dataset.table]&lt;/code&gt;  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Querying Multiple Repeated Fields&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;FLATTEN operator unrolls multiple repeated fields  &lt;ul&gt;
&lt;li&gt;One record for each of the values  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Example: &lt;code&gt;SELECT * FROM (FLATTEN ([dataset.table], city))&lt;/code&gt;  &lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Lab 7.2&lt;/h3&gt;
&lt;p&gt;BigQuery will automatically flatten a single repeated field (in this case, "city").&lt;br /&gt;
Additional (independent) repeated fields in a query each require the use of the FLATTEN statement.&lt;br /&gt;
A query on 5 independently repeating fields will require 4 FLATTEN statements.  &lt;/p&gt;
&lt;h2&gt;Nested Repeated Fields&lt;/h2&gt;
&lt;p&gt;"type": "RECORD"&lt;br /&gt;
"mode": "REPEATED"  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;BigQuery supports importing and exporting nested repeated fields in JSON and AVRO files  &lt;/li&gt;
&lt;li&gt;Combine nested and repeated fields to denormalize a &lt;em&gt;one-to-many&lt;/em&gt; relationship  &lt;/li&gt;
&lt;li&gt;Useful as a mechanism to organize related information  &lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Example: JSON Nested Repeated Schema&lt;/h3&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;{&amp;quot;name&amp;quot;: &amp;quot;author&amp;quot;, &amp;quot;type&amp;quot;: &amp;quot;string&amp;quot;, &amp;quot;mode&amp;quot;: &amp;quot;REQUIRED&amp;quot;},  
{&amp;quot;name&amp;quot;: &amp;quot;book&amp;quot;, &amp;quot;type&amp;quot;: &amp;quot;RECORD&amp;quot;, &amp;quot;mode&amp;quot;: &amp;quot;REPEATED&amp;quot;, &amp;quot;fields&amp;quot;:  
    [  
        {&amp;quot;name&amp;quot;: &amp;quot;title&amp;quot;, &amp;quot;type&amp;quot;: &amp;quot;string&amp;quot;, &amp;quot;mode&amp;quot;: &amp;quot;REQUIRED&amp;quot;},  
        {&amp;quot;name&amp;quot;: &amp;quot;checked_out&amp;quot;, &amp;quot;type&amp;quot;: &amp;quot;timestamp&amp;quot;, &amp;quot;mode&amp;quot;: &amp;quot;REPEATED&amp;quot;}  
    ]  
},  
{&amp;quot;name&amp;quot;: &amp;quot;citiesLived&amp;quot;, &amp;quot;type&amp;quot;: &amp;quot;RECORD&amp;quot;, &amp;quot;mode&amp;quot;: &amp;quot;REPEATED&amp;quot;, &amp;quot;fields&amp;quot;:  
    [  
        {&amp;quot;name&amp;quot;: &amp;quot;place&amp;quot;, &amp;quot;type&amp;quot;: &amp;quot;string&amp;quot;},  
        {&amp;quot;name&amp;quot;: &amp;quot;yearsLived&amp;quot;, &amp;quot;type&amp;quot;: &amp;quot;integer&amp;quot;, &amp;quot;mode&amp;quot;: &amp;quot;REPEATED&amp;quot;}  
    ]  
}  
&lt;/pre&gt;&lt;/div&gt;


&lt;h3&gt;Example: JSON Nested Repeated Data&lt;/h3&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;{&amp;quot;author&amp;quot;: &amp;quot;melville&amp;quot;, &amp;quot;book&amp;quot;: [{&amp;quot;title&amp;quot;: &amp;quot;Moby Dick&amp;quot;, &amp;quot;checked_out&amp;quot;: [&amp;quot;2014-12-12 14:23&amp;quot;, &amp;quot;2013-04-03 12:13&amp;quot;]}], &amp;quot;citiesLived&amp;quot;: [{&amp;quot;place&amp;quot;: &amp;quot;Denver, CO&amp;quot;, &amp;quot;yearsLived&amp;quot; :[&amp;quot;1986&amp;quot;, &amp;quot;1987&amp;quot;]}]}  
{&amp;quot;author&amp;quot;: &amp;quot;hardy&amp;quot;, &amp;quot;book&amp;quot;: [ {&amp;quot;title&amp;quot;: &amp;quot;Return of the Native&amp;quot;, &amp;quot;checked_out&amp;quot;: [&amp;quot;1984-05-30 12:12&amp;quot;, &amp;quot;1986-03-12 00:00&amp;quot;, &amp;quot;1992-05-03 04:32&amp;quot;] }, {&amp;quot;title&amp;quot;: &amp;quot;The Mayor of Casterbridge&amp;quot;, &amp;quot;checked_out&amp;quot;: [&amp;quot;1983-06-23 12:12&amp;quot;, &amp;quot;1986-03-12 00:00&amp;quot;, &amp;quot;1992-05-03 04:32&amp;quot;] } ], &amp;quot;citiesLived&amp;quot;: [ {&amp;quot;place&amp;quot;: &amp;quot;Austin, TX&amp;quot;, &amp;quot;yearsLived&amp;quot; : [&amp;quot;1982&amp;quot;, &amp;quot;1983&amp;quot;, &amp;quot;1984&amp;quot;] }, {&amp;quot;place&amp;quot;: &amp;quot;Dublin, CA&amp;quot;, &amp;quot;yearsLived&amp;quot; : [&amp;quot;1992&amp;quot;, &amp;quot;1999&amp;quot;, &amp;quot;2000&amp;quot;]}]}  
{&amp;quot;author&amp;quot;: &amp;quot;koontz&amp;quot;, &amp;quot;book&amp;quot;: [ {&amp;quot;title&amp;quot;: &amp;quot;Velocity&amp;quot;, &amp;quot;checked_out&amp;quot;: [&amp;quot;1990-06-10 12:10&amp;quot;, &amp;quot;2000-03-11 10:00&amp;quot;, &amp;quot;1992-05-03 04:32&amp;quot;] }, {&amp;quot;title&amp;quot;: &amp;quot;Intensity&amp;quot;, &amp;quot;checked_out&amp;quot;: [&amp;quot;2003-06-23 02:12&amp;quot;, &amp;quot;2004-03-12 20:00&amp;quot;, &amp;quot;1992-05-03 04:32&amp;quot;]}]}  
&lt;/pre&gt;&lt;/div&gt;


&lt;h3&gt;Querying Nested Repeated Fields&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;A query such as &lt;code&gt;SELECT * ...&lt;/code&gt; produces an error:  &lt;ul&gt;
&lt;li&gt;Cannot output multiple independently repeated fields at the same time  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Example: &lt;code&gt;SELECT author, book.title, book.checked_out FROM [dataset.table]&lt;/code&gt;  &lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Using the WITHIN Keyword&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;WITHIN keyword works with aggregate functions  &lt;/li&gt;
&lt;li&gt;Example:  &lt;ul&gt;
&lt;li&gt;&lt;code&gt;SELECT fullName, COUNT(children.name) WITHIN RECORD FROM [dataset.tableId]&lt;/code&gt;  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;code&gt;WITHIN RECORD&lt;/code&gt;  &lt;ul&gt;
&lt;li&gt;Aggregates data in the repeated values within the record  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;code&gt;WITHIN &amp;lt;node&amp;gt;&lt;/code&gt;  &lt;ul&gt;
&lt;li&gt;Aggregates data in the repeated values within a node  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Example:  &lt;ul&gt;
&lt;li&gt;&lt;code&gt;SELECT fullName, count(citiesLived.place) WITHIN RECORD, citiesLived.place, count(citiesLived.yearsLived) WITHIN citiesLived FROM [dataset.tableId]&lt;/code&gt;  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;要瞭解 Record 的結構層級，才能比較有效的使用 Within。  &lt;/p&gt;
&lt;h3&gt;Lab 7.3&lt;/h3&gt;
&lt;p&gt;使用 &lt;code&gt;REAPETED&lt;/code&gt; and &lt;code&gt;NETSTED&lt;/code&gt; Record 的時候，&lt;br /&gt;
仍然要注意只能同時有一個 REAPETED field，&lt;br /&gt;
否則就得使用 &lt;code&gt;FLATTEN&lt;/code&gt;  &lt;/p&gt;
&lt;hr /&gt;
&lt;h1&gt;Module 8: Query Performance&lt;/h1&gt;
&lt;h2&gt;JOIN and GROUP BY – How They Affect Performance&lt;/h2&gt;
&lt;h3&gt;JOIN&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;When possible, avoid CROSS JOIN  &lt;/li&gt;
&lt;li&gt;Each row from first table is joined to every row in second table returning large amounts of data  &lt;/li&gt;
&lt;li&gt;May result in “resources exceeded” errors  &lt;/li&gt;
&lt;li&gt;Window functions often more efficient  &lt;ul&gt;
&lt;li&gt;例如：一個小時做一個 window  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;The 8MB right-side table join limit no longer applies.  &lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;GROUP BY&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Use &lt;code&gt;GROUP BY&lt;/code&gt; when the number of distinct groups is small (low cardinality)  &lt;ul&gt;
&lt;li&gt;Aggregation of data performed in shards  &lt;/li&gt;
&lt;li&gt;Low cardinality means shards do not shuffle data  &lt;/li&gt;
&lt;li&gt;High performance  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Large &lt;code&gt;GROUP BY&lt;/code&gt; is less optimal  &lt;ul&gt;
&lt;li&gt;建議不要做，但如果不得已的話還是可以做啦。  &lt;/li&gt;
&lt;li&gt;High cardinality requires aggregation performed by multiple shards  &lt;/li&gt;
&lt;li&gt;Shards produce hash key for each value and shuffle data  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;ROLLUP - Legacy SQL&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Use &lt;code&gt;ROLLUP&lt;/code&gt; function in legacy SQL for large GROUP BY  &lt;/li&gt;
&lt;li&gt;Adds extra rows to result that represent partial aggregations  &lt;/li&gt;
&lt;li&gt;The fields in the GROUP BY must be in the SELECT (declares which columns to process).  &lt;/li&gt;
&lt;/ul&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;SELECT  
    year,  
    is_male,  
    COUNT(1) AS COUNT  
FROM  
    publicdata:samples.natality  
WHERE  
    year &amp;gt;= 2000 AND year &amp;lt;=2002  
GROUP BY  
    ROLLUP(year, is_male)  
ORDER BY  
    year, is_male  
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;+------+---------+----------+  
| year | is_male | count    |  
+------+---------+----------+  
| NULL | NULL    | 12122730 |  
| 2000 | NULL    | 4063823  |  
| 2000 | false   | 1984255  |  
| 2000 | true    | 2079568  |  
| 2001 | NULL    | 4031531  |  
| 2001 | false   | 1970770  |  
| ...                       |  
| 2002 | true    | 2060857  |  
+------+---------+----------+  
&lt;/pre&gt;&lt;/div&gt;


&lt;h3&gt;Example: Large GROUP BY&lt;/h3&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;SELECT  
    LogEdits,  
    COUNT(contributor_id) Contributors  
FROM (  
    SELECT  
        contributor_id,  
        INTEGER(LOG10(COUNT(*))) LogEdits  
    FROM  
        [publicdata:samples.wikipedia]  
    GROUP BY contributor_id  
)  
GROUP BY LogEdits  
ORDER BY LogEdits DESC  
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;Table Decorators&lt;/h2&gt;
&lt;h3&gt;BigQuery Table Decorators&lt;/h3&gt;
&lt;p&gt;只有要查有異動的資料的話，使用 Decorators 就好，可以節省資料量。&lt;br /&gt;
不過 7 天之內要做，因為只會保留 7 天。  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Use to perform the most cost-effective query of a subset of your data  &lt;/li&gt;
&lt;li&gt;Table decorators can be used whenever a table is read  &lt;/li&gt;
&lt;li&gt;Copying a table, exporting a table, or listing data  &lt;/li&gt;
&lt;li&gt;Can also be used to undelete a table within 2 days on a best-effort basis  &lt;/li&gt;
&lt;li&gt;Currently supported in legacy SQL only  &lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Table Decorator Types&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Snapshot decorators  &lt;ul&gt;
&lt;li&gt;&lt;code&gt;@&amp;lt;time&amp;gt;&lt;/code&gt;  &lt;/li&gt;
&lt;li&gt;Time must be within last 7 days  &lt;/li&gt;
&lt;li&gt;&lt;code&gt;@0&lt;/code&gt; references oldest snapshot  &lt;/li&gt;
&lt;li&gt;Relative time is negative  &lt;/li&gt;
&lt;li&gt;Absolute time is positive  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Range decorators  &lt;ul&gt;
&lt;li&gt;&lt;code&gt;@&amp;lt;time 1&amp;gt;-&amp;lt;time 2&amp;gt;&lt;/code&gt;  &lt;/li&gt;
&lt;li&gt;Time must be within last 7 days  &lt;/li&gt;
&lt;li&gt;References data between &lt;code&gt;&amp;lt;time 1&amp;gt;&lt;/code&gt; and &lt;code&gt;&amp;lt;time 2&amp;gt;&lt;/code&gt;  &lt;/li&gt;
&lt;li&gt;Time 2 is optional and defaults to ‘now’  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Example: Snapshot Table Decorator&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://blog.gdeltproject.org/using-bigquery-table-decorators-to-lower-query-cost/"&gt;http://blog.gdeltproject.org/using-bigquery-table-decorators-to-lower-query-cost/&lt;/a&gt;  &lt;/li&gt;
&lt;li&gt;&lt;code&gt;@-14400000&lt;/code&gt; - is a reference to a snapshot of the table at -14400000 milliseconds since the current time  &lt;ul&gt;
&lt;li&gt;14400000 milliseconds == 14400 seconds == 4 hours  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;SELECT count(*)  
FROM [publicdata:samples.shakespe are@-14400000]  
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;Wildcards&lt;/h2&gt;
&lt;h3&gt;Wildcard Functions - Legacy SQL&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Cost-effective way to query data from a set of “sharded” tables  &lt;ul&gt;
&lt;li&gt;Only the tables that match the wildcard are accessed  &lt;/li&gt;
&lt;li&gt;Limits BigQuery data charges  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Equivalent to UNION of tables matched by wildcard  &lt;/li&gt;
&lt;li&gt;Limits:  &lt;ul&gt;
&lt;li&gt;&lt;em&gt;No query can reference more than 1,000 tables (even via views)&lt;/em&gt;  &lt;/li&gt;
&lt;li&gt;The query planner collects table metadata which can have a performance impact for a large number of shards  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;code&gt;TABLE_DATE_RANGE(prefix, timestamp1, timestamp2)&lt;/code&gt;&lt;/h4&gt;
&lt;p&gt;Queries daily tables that overlap with the time range between &lt;code&gt;&amp;lt;timestamp1&amp;gt;&lt;/code&gt; and &lt;code&gt;&amp;lt;timestamp2&amp;gt;&lt;/code&gt;.&lt;br /&gt;
Table names must have the following format: &lt;code&gt;&amp;lt;prefix&amp;gt;&amp;lt;day&amp;gt;&lt;/code&gt;, where &lt;code&gt;&amp;lt;day&amp;gt;&lt;/code&gt; is in the format YYYYMMDD.&lt;br /&gt;
You can use date and time functions to generate the timestamp parameters.&lt;br /&gt;
For example:&lt;br /&gt;
+ &lt;code&gt;TIMESTAMP('2012-10-01 02:03:04')&lt;/code&gt;&lt;br /&gt;
+ &lt;code&gt;DATE_ADD(CURRENT_TIMESTAMP(), -7, 'DAY')&lt;/code&gt;  &lt;/p&gt;
&lt;h4&gt;&lt;code&gt;TABLE_DATE_RANGE_STRICT(prefix, timestamp1, timestamp2)&lt;/code&gt;&lt;/h4&gt;
&lt;p&gt;This function is equivalent to &lt;code&gt;TABLE_DATE_RANGE&lt;/code&gt;.&lt;br /&gt;
The only difference is that if any daily table is missing in the sequence,&lt;br /&gt;
&lt;code&gt;TABLE_DATE_RANGE_STRICT&lt;/code&gt; fails and returns a &lt;code&gt;Not Found: Table &amp;lt;table_name&amp;gt; error&lt;/code&gt;.  &lt;/p&gt;
&lt;h4&gt;&lt;code&gt;TABLE_QUERY(dataset, expr)&lt;/code&gt;&lt;/h4&gt;
&lt;p&gt;Queries tables whose names match the supplied expr.&lt;br /&gt;
The expr parameter must be represented as a string and must contain an expression to evaluate.&lt;br /&gt;
For example, &lt;code&gt;'length(table_id) &amp;lt; 3'&lt;/code&gt;.  &lt;/p&gt;
&lt;h4&gt;Wildcard Function Examples&lt;/h4&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;SELECT ...  
FROM  
    TABLE_DATE_RANGE(  
        dataset.log,  
        TIMESTAMP(&amp;#39;2015-01-01&amp;#39;),  
        TIMESTAMP(&amp;#39;2015-01-03&amp;#39;)  
     )  
&lt;/pre&gt;&lt;/div&gt;


&lt;h3&gt;Wildcard Tables - Standard SQL&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Query multiple tables using concise SQL statements  &lt;/li&gt;
&lt;li&gt;Wildcard table represents union of all tables that match the wildcard expression (like wildcard functions)  &lt;/li&gt;
&lt;li&gt;Useful when dataset contains multiple, similarly named tables with compatible schemas  &lt;/li&gt;
&lt;li&gt;Each row in wildcard table contains special column containing value matched by wildcard character  &lt;/li&gt;
&lt;li&gt;Example:  &lt;ul&gt;
&lt;li&gt;&lt;code&gt;FROM bigquery-public-data.noaa_gsod.gsod*&lt;/code&gt;  &lt;ul&gt;
&lt;li&gt;Matches all tables in noaa_gsod that begin with string 'gsod'  &lt;/li&gt;
&lt;li&gt;character is required (single, double quotes are invalid)  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Longer prefixes generally perform better than shorter prefixes  &lt;ul&gt;
&lt;li&gt;For example: &lt;code&gt;.gsod200*&lt;/code&gt; versus &lt;code&gt;.*&lt;/code&gt;  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Partitions&lt;/h2&gt;
&lt;h3&gt;Table Partitioning - Current Approach&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Time-partitioned tables are cost-effective way to manage data, write queries spanning multiple days, months, years  &lt;/li&gt;
&lt;li&gt;Create tables with time-based partitions and BigQuery automatically loads data in correct partition  &lt;ul&gt;
&lt;li&gt;Declare the table as partitioned at creation time using &lt;code&gt;--time_partitioning_type&lt;/code&gt; flag  &lt;/li&gt;
&lt;li&gt;To create partitioned table with expiration time for data, use &lt;code&gt;time_partitioning_expiration&lt;/code&gt; flag  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;To query partitioned table, provide date or range of dates and query processes data for interval specified  &lt;/li&gt;
&lt;li&gt;Only data scanned is in partitions specified by interval  &lt;/li&gt;
&lt;li&gt;Queries are more performant, cheaper  &lt;/li&gt;
&lt;li&gt;Currently only supported by legacy SQL  &lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;Example - Table Partitioning&lt;/h4&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;SELECT  
    ...  
FROM  
    sales  
WHERE  
    _PARTITIONTIME BETWEEN TIMESTAMP(&amp;quot;20160101&amp;quot;) AND TIMESTAMP(&amp;quot;20160131&amp;quot;)  
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;Query Performance Tips&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Denormalize tables for performance  &lt;/li&gt;
&lt;li&gt;Select only needed columns - &lt;code&gt;Do not use Select *&lt;/code&gt;  &lt;/li&gt;
&lt;li&gt;Schedule batch queries at off-peak hours using jobs  &lt;/li&gt;
&lt;li&gt;Use caching when possible  &lt;ul&gt;
&lt;li&gt;Caching is best effort  &lt;/li&gt;
&lt;li&gt;If table data changes, cache is invalidated  &lt;/li&gt;
&lt;li&gt;Use jobs.getQueryResults to page through cached query results in a temporary table (no charge)  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Try to use ORDER BY and LIMIT in outermost queries  &lt;ul&gt;
&lt;li&gt;&lt;code&gt;LIMIT&lt;/code&gt; is applied to results by Master  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Build queries from the inside out by using subqueries  &lt;ul&gt;
&lt;li&gt;Filter data in subqueries  &lt;/li&gt;
&lt;li&gt;Perform arithmetic, ordering, case logic in outer query  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Use queries to create materialized intermediate tables  &lt;ul&gt;
&lt;li&gt;Create subset of complex data in destination table  &lt;/li&gt;
&lt;li&gt;Partially aggregate data in destination table  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Move heavyweight filters, such as regexp, to the end  &lt;/li&gt;
&lt;li&gt;Avoid grouping on unbounded possible values  &lt;ul&gt;
&lt;li&gt;Example: Web logs with arbitrary GET parameters in the suffix  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Consider using IF/CASE instead of self-joins because IF/CASE has lower processing overhead  &lt;ul&gt;
&lt;li&gt;Self-joins require multiple disk reads  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Apply WHERE filters prior to JOINs  &lt;ul&gt;
&lt;li&gt;Predicate pushdown  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;hr /&gt;
&lt;h1&gt;Module 9: Troubleshooting Errors&lt;/h1&gt;
&lt;h2&gt;Error Categories&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Request encoding errors  &lt;ul&gt;
&lt;li&gt;Associated with the query request – Invalid query syntax, and so on  &lt;/li&gt;
&lt;li&gt;Request Body 有錯誤，通常是由 Google 的 API 處理  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Application errors  &lt;ul&gt;
&lt;li&gt;Errors associated in processing the request  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;HTTP transport layer errors  &lt;ul&gt;
&lt;li&gt;Programmatic communication errors using API  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Request Encoding Errors&lt;/h2&gt;
&lt;p&gt;在 Query History 裡頭，&lt;br /&gt;
點開有錯誤的 Query 可以看到 Error code，&lt;br /&gt;
可以點選超連結去察看該 Error 詳細的錯誤原因。  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Incomplete syntax  &lt;/li&gt;
&lt;li&gt;Missing or invalid objects  &lt;/li&gt;
&lt;li&gt;Missing columns in GROUP BY (for aggregations)  &lt;/li&gt;
&lt;li&gt;Incorrect or missing punctuation  &lt;/li&gt;
&lt;li&gt;Misspellings  &lt;/li&gt;
&lt;li&gt;Ambiguous field references  &lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Resource Errors&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Intended limitations exist in the query execution engine to protect resources  &lt;ul&gt;
&lt;li&gt;Can cause well-formed queries to fail  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Two classes of resource errors  &lt;ul&gt;
&lt;li&gt;Result too large  &lt;/li&gt;
&lt;li&gt;Resources exceeded  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Result Too Large&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;BigQuery limits result sets to approximately 128MB compressed  &lt;ul&gt;
&lt;li&gt;Queries returning larger results cannot fit into response  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Result too large:  &lt;ul&gt;
&lt;li&gt;Commonly thrown on queries that use an ORDER BY with large cardinality  &lt;/li&gt;
&lt;li&gt;Can happen in multiple stages of the serving tree  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;Handling Result Too Large Errors&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Use filters to limit the result set  &lt;/li&gt;
&lt;li&gt;Use LIMIT clause  &lt;/li&gt;
&lt;li&gt;Remove ORDER BY for large datasets (order by without limit is meaningless)  &lt;/li&gt;
&lt;li&gt;Specify destination table and use &lt;code&gt;allowLargeResults&lt;/code&gt; flag  &lt;ul&gt;
&lt;li&gt;Impacts query performance  &lt;/li&gt;
&lt;li&gt;可以保證資料都出得來，但效能會有影響，因為有些前置動作會比較不一樣，可以視需求決定要不要用這個 flag  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Limitations of &lt;code&gt;allowLargeResults&lt;/code&gt; flag  &lt;ul&gt;
&lt;li&gt;Cannot specify top-level ORDER BY, TOP or LIMIT clause  &lt;ul&gt;
&lt;li&gt;Negates benefit of allowLargeResults because query output is no longer computed in parallel  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Using allowLargeResults flag with ORDER BY can cause resources exceeded errors  &lt;ul&gt;
&lt;li&gt;Master applies final sorting  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Using allowLargeResults with window functions requires PARTITION BY clause  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Resources Exceeded&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Resources exceeded error issued if a query exceeds the memory limit on a single shard  &lt;ul&gt;
&lt;li&gt;Once data is read from disk, processing is done in memory  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Most common on:  &lt;ul&gt;
&lt;li&gt;ORDER BY queries with large numbers of distinct values  &lt;/li&gt;
&lt;li&gt;JOINs with more outputs than inputs  &lt;/li&gt;
&lt;li&gt;Aggregations that require memory proportional to the number of input values  &lt;/li&gt;
&lt;li&gt;Queries where data is heavily skewed toward one key value  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;Handling Resources Exceeded Errors&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;If possible, limit use of &lt;code&gt;ORDER BY&lt;/code&gt;  &lt;/li&gt;
&lt;li&gt;Use aggregation functions that generate fewer output results than input rows  &lt;/li&gt;
&lt;li&gt;Avoid JOINs that generate more outputs than inputs  &lt;/li&gt;
&lt;li&gt;Avoid queries that create data skew  &lt;/li&gt;
&lt;li&gt;Queries on "guest" IDs or null values  &lt;/li&gt;
&lt;li&gt;No rule that works for every case  &lt;/li&gt;
&lt;li&gt;或者先用一個 Query 把結果存成比較小的 Table (Destination Table)，再拿這個 Table 來 Query  &lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;HTTP Errors and Responses&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;All BigQuery API calls are HTTP requests  &lt;/li&gt;
&lt;li&gt;All HTTP requests return status codes  &lt;/li&gt;
&lt;li&gt;Codes between 200 and 299 are success codes  &lt;/li&gt;
&lt;li&gt;HTTP error codes are between 400 and 599  &lt;/li&gt;
&lt;li&gt;BigQuery returns a standard JSON response on error  &lt;/li&gt;
&lt;li&gt;可以參考這個網頁：&lt;a href="https://cloud.google.com/bigquery/troubleshooting-errors"&gt;https://cloud.google.com/bigquery/troubleshooting-errors&lt;/a&gt;  &lt;/li&gt;
&lt;/ul&gt;
&lt;hr /&gt;
&lt;h1&gt;Module 10: Access Control&lt;/h1&gt;
&lt;h2&gt;Access Control Lists&lt;/h2&gt;
&lt;p&gt;從 Dataset 這個層面去做設定  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;ACLs define permissions given to a role (or grantee) for a target (project/dataset)  &lt;/li&gt;
&lt;li&gt;ACLs consist of one or more entries that grant permission to a role (or grantee)  &lt;/li&gt;
&lt;li&gt;Permissions define the actions that can be performed against a project or dataset  &lt;ul&gt;
&lt;li&gt;Scope defines to whom the permission applies  &lt;/li&gt;
&lt;li&gt;參考 &lt;a href="https://developers.google.com/apis-explorer/#p/bigquery/v2/"&gt;https://developers.google.com/apis-explorer/#p/bigquery/v2/&lt;/a&gt;  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Roles:  &lt;ul&gt;
&lt;li&gt;Project roles – Users can run jobs or manage the project  &lt;/li&gt;
&lt;li&gt;Dataset roles – Define user access to datasets in a project  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Project Roles&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Granted/revoked using Cloud Platform Console  &lt;/li&gt;
&lt;li&gt;Roles are assigned by email address for:  &lt;ul&gt;
&lt;li&gt;Individual users  &lt;/li&gt;
&lt;li&gt;Groups  &lt;/li&gt;
&lt;li&gt;Service accounts  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Project owners can modify project roles  &lt;ul&gt;
&lt;li&gt;Automatically granted to project creator  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;要授權給一群 Google User 的話，可以考慮用 Google Groups 來做這件事，只要授權給一整個 Group 就行了，不用一個一個加。  &lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Permissions for Project Roles&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Viewer  &lt;ul&gt;
&lt;li&gt;Can start a job - Dataset roles also required depending on job type  &lt;/li&gt;
&lt;li&gt;List and get all jobs they started  &lt;/li&gt;
&lt;li&gt;Granted READER dataset role by default for new datasets in project  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Editor  &lt;ul&gt;
&lt;li&gt;Same as Viewer, plus:  &lt;ul&gt;
&lt;li&gt;Can create new dataset in project  &lt;/li&gt;
&lt;li&gt;Is granted WRITER role by default for new datasets in project  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Owner  &lt;ul&gt;
&lt;li&gt;Same as Editor, plus:  &lt;ul&gt;
&lt;li&gt;Can list all datasets in the project  &lt;/li&gt;
&lt;li&gt;Can delete any dataset in the project  &lt;/li&gt;
&lt;li&gt;Can list and get all jobs run  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Dataset Roles&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;The project ACL becomes default ACL for datasets in the project  &lt;ul&gt;
&lt;li&gt;Default access can be overridden on a per-dataset basis  &lt;/li&gt;
&lt;li&gt;Tables inherit ACLs from dataset  &lt;ul&gt;
&lt;li&gt;ACLs cannot be configured on tables  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Dataset ACLs allow resource separation  &lt;ul&gt;
&lt;li&gt;No need for additional clusters and data duplication  &lt;/li&gt;
&lt;li&gt;Saves money, simplifies operations  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Dataset roles are granted or revoked using:  &lt;ul&gt;
&lt;li&gt;The BigQuery web UI  &lt;/li&gt;
&lt;li&gt;Using the 'Share dataset' option  &lt;/li&gt;
&lt;li&gt;The BigQuery API  &lt;/li&gt;
&lt;li&gt;Using Datasets:&lt;code&gt;update&lt;/code&gt;  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Roles are assigned by email address to:  &lt;ul&gt;
&lt;li&gt;Single user  &lt;/li&gt;
&lt;li&gt;Google Groups  &lt;/li&gt;
&lt;li&gt;Predefined group of users, such as all users, or a group of users with same project role  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Permissions for Dataset Roles&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Reader  &lt;ul&gt;
&lt;li&gt;Can read, query, copy or export tables in the dataset  &lt;ul&gt;
&lt;li&gt;Can call get on the dataset and tables in dataset  &lt;/li&gt;
&lt;li&gt;Can call list on table data for tables in the dataset  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Writer  &lt;ul&gt;
&lt;li&gt;Same as READER, plus:  &lt;ul&gt;
&lt;li&gt;Can edit or append data in the dataset  &lt;ul&gt;
&lt;li&gt;Can call insert, insertAll, update or delete  &lt;/li&gt;
&lt;li&gt;Can use tables in the dataset as destinations for load, copy or query jobs  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Owner  &lt;ul&gt;
&lt;li&gt;Same as WRITER, plus:  &lt;ul&gt;
&lt;li&gt;Can call update on the dataset  &lt;/li&gt;
&lt;li&gt;Can call delete on the dataset  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Note: A dataset must have at least one entity with the OWNER role.  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Applying Views for Row-Level Security&lt;/h2&gt;
&lt;h3&gt;Row-Level Security&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Not natively supported  &lt;ul&gt;
&lt;li&gt;Define a view to give access to a specific view of the data  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;View – a BigQuery SQL query that limits the rows and columns (a virtual table) that a user can see  &lt;/li&gt;
&lt;li&gt;BigQuery views are re-executed every time the view is queried  &lt;/li&gt;
&lt;li&gt;Create view in dataset separate from underlying table’s dataset and assign ACLs to both datasets  &lt;/li&gt;
&lt;li&gt;Row-Leve Security Scenario  &lt;ul&gt;
&lt;li&gt;可以透過 View 來 select 只想被 share 出去的欄位，不需要開放整個 table 的權限。  &lt;/li&gt;
&lt;li&gt;從一個使用者 A 沒有權限的 dataset 把一個 View 存到使用者 A 有權限的 dataset，使用者 A 仍然無法使用該 View 得到他沒有權限的 dataset 的資料。  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Identity and Access Management&lt;/h2&gt;
&lt;h3&gt;Benefits of IAM for BigQuery&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Admins can isolate permissions to BigQuery  &lt;ul&gt;
&lt;li&gt;For example, BigQuery roles have no authority to manage Compute Engine virtual machines  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Consistent IAM controls across all GCP products  &lt;/li&gt;
&lt;li&gt;Narrow permissions allow more fine-grained control  &lt;/li&gt;
&lt;li&gt;Backwards compatible  &lt;ul&gt;
&lt;li&gt;Legacy project permissions are preserved, and the familiar UI, API, and CLI will continue to work as before with minimal changes  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Organization Node (Beta)&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Organization node is root node for Google Cloud resources  &lt;/li&gt;
&lt;li&gt;2 organization roles:  &lt;ul&gt;
&lt;li&gt;Organization Admin - Control over all cloud resources  &lt;/li&gt;
&lt;li&gt;Project Creator - Controls project creation  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;IAM Resource Hierarchy&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;A policy is set on a resource  &lt;ul&gt;
&lt;li&gt;Each policy contains: Set of roles, role members  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Resources inherit policies from parent  &lt;ul&gt;
&lt;li&gt;Resource policies are a union of parent and resource  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;If parent policy less restrictive, overrides more restrictive resource policy  &lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;IAM Roles - Curated Roles&lt;/h3&gt;
&lt;p&gt;The “can do what” part is defined by an IAM role.&lt;br /&gt;
An IAM role is a collection of permissions.&lt;br /&gt;
Most of the time to do any meaningful operations you need more than 1 permission.&lt;br /&gt;
For example to manage instances in a project, you need to create, delete, start, stop and change an instance.&lt;br /&gt;
So the permissions are grouped together into a role to make it easier to manage.  &lt;/p&gt;
&lt;h3&gt;BigQuery IAM Roles&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;User  &lt;ul&gt;
&lt;li&gt;Runs jobs such as queries  &lt;/li&gt;
&lt;li&gt;Can browse the project to see what data is available, but does not have access to it by default  &lt;/li&gt;
&lt;li&gt;Can be assigned at project level or higher  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Admin  &lt;ul&gt;
&lt;li&gt;All BigQuery related permissions - Access to read, write, delete all data, view jobs and/or cancel them  &lt;/li&gt;
&lt;li&gt;Can be assigned at project level or higher  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Data viewer  &lt;ul&gt;
&lt;li&gt;Can view all datasets and all data within those datasets within the scope of the role  &lt;/li&gt;
&lt;li&gt;Can be assigned at dataset level or higher  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Data editor  &lt;ul&gt;
&lt;li&gt;Can edit all datasets and all data within those datasets within the scope of the role  &lt;/li&gt;
&lt;li&gt;Can be assigned at dataset level or higher  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;hr /&gt;
&lt;h1&gt;Module 11: Exporting Data&lt;/h1&gt;
&lt;h2&gt;Exporting Data&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Why export data?  &lt;ul&gt;
&lt;li&gt;Using data with third-party tools  &lt;/li&gt;
&lt;li&gt;Snapshots  &lt;/li&gt;
&lt;li&gt;Backups  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Export using:  &lt;ul&gt;
&lt;li&gt;Web UI  &lt;/li&gt;
&lt;li&gt;CLI  &lt;/li&gt;
&lt;li&gt;REST API  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Limitations  &lt;ul&gt;
&lt;li&gt;Export up to 1 GB of data per file (multiple file export supported)  &lt;/li&gt;
&lt;li&gt;Daily limit: 1,000 exports per day, up to 10 TB  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;ACL requirements for exporting data:  &lt;ul&gt;
&lt;li&gt;BigQuery: Dataset-level READER access  &lt;/li&gt;
&lt;li&gt;Google Cloud Storage: WRITE access to Google Cloud Storage bucket(s)  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Export Configuration Options&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Two aspects: Format and compression  &lt;ul&gt;
&lt;li&gt;destinationFormat  &lt;ul&gt;
&lt;li&gt;CSV  &lt;/li&gt;
&lt;li&gt;JSON  &lt;/li&gt;
&lt;li&gt;Avro  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;compression  &lt;ul&gt;
&lt;li&gt;GZIP  &lt;/li&gt;
&lt;li&gt;NONE  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Notes:  &lt;ul&gt;
&lt;li&gt;AVRO cannot be used with GZIP compression  &lt;/li&gt;
&lt;li&gt;Nested and repeated data cannot be exported in CSV format  &lt;/li&gt;
&lt;li&gt;Defaults: CSV with no compression  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;AVRO Export Format&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Exported files are Avro container files  &lt;/li&gt;
&lt;li&gt;Each row is represented as an Avro record  &lt;ul&gt;
&lt;li&gt;Nested data is represented by nested record objects  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;em&gt;REQUIRED&lt;/em&gt; fields represented as corresponding Avro types  &lt;ul&gt;
&lt;li&gt;For example: An INTEGER type maps to an Avro LONG type  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;em&gt;NULLABLE&lt;/em&gt; fields represented as Avro Union of corresponding type and "null"  &lt;/li&gt;
&lt;li&gt;&lt;em&gt;REPEATED&lt;/em&gt; fields are represented as Avro arrays  &lt;/li&gt;
&lt;li&gt;&lt;em&gt;TIMESTAMP&lt;/em&gt; data types represented as Avro LONG types  &lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Running Export Jobs&lt;/h2&gt;
&lt;h3&gt;CLI&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;bq extract&lt;/code&gt; - Perform an extract operation against &lt;code&gt;source_table&lt;/code&gt; into &lt;code&gt;destination_uris&lt;/code&gt;  &lt;ul&gt;
&lt;li&gt;&lt;code&gt;bq extract &amp;lt;source_table&amp;gt; &amp;lt;destination_uris&amp;gt;&lt;/code&gt;  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Web UI&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;操作很簡單  &lt;/li&gt;
&lt;li&gt;有很大的檔案的話，儘量不要用 WebUI export，因為有可能中間被中斷就得重下載。請丟到 GCS 再去 download 下來  &lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Configuration Example&lt;/h3&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="err"&gt;jobData&lt;/span&gt; &lt;span class="err"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;  
    &lt;span class="err"&gt;&amp;#39;projectId&amp;#39;:&lt;/span&gt; &lt;span class="err"&gt;projectId,&lt;/span&gt;  
    &lt;span class="err"&gt;&amp;#39;configuration&amp;#39;:&lt;/span&gt; &lt;span class="err"&gt;{&lt;/span&gt;  
        &lt;span class="err"&gt;&amp;#39;extract&amp;#39;:&lt;/span&gt; &lt;span class="err"&gt;{&lt;/span&gt;  
            &lt;span class="err"&gt;&amp;#39;sourceTable&amp;#39;:&lt;/span&gt; &lt;span class="err"&gt;{&lt;/span&gt;  
                &lt;span class="err"&gt;&amp;#39;projectId&amp;#39;:&lt;/span&gt; &lt;span class="err"&gt;projectId,&lt;/span&gt;  
                &lt;span class="err"&gt;&amp;#39;datasetId&amp;#39;:&lt;/span&gt; &lt;span class="err"&gt;datasetId,&lt;/span&gt;  
                &lt;span class="err"&gt;&amp;#39;tableId&amp;#39;:&lt;/span&gt; &lt;span class="err"&gt;tableId&lt;/span&gt;  
            &lt;span class="p"&gt;}&lt;/span&gt;&lt;span class="err"&gt;,&lt;/span&gt;  
            &lt;span class="err"&gt;&amp;#39;destinationUris&amp;#39;:&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="err"&gt;&amp;#39;gs://&amp;lt;bucket&amp;gt;/&amp;lt;file&amp;gt;&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="err"&gt;,&lt;/span&gt;  
            &lt;span class="err"&gt;&amp;#39;destinationFormat&amp;#39;:&lt;/span&gt; &lt;span class="err"&gt;&amp;#39;NEWLINE_DELIMITED_JSON&amp;#39;&lt;/span&gt;  
        &lt;span class="err"&gt;}&lt;/span&gt; &lt;span class="err"&gt;...&lt;/span&gt;  
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;Wildcard Exports&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;If export is larger than 1GB, use a wildcard to partition the output into multiple files  &lt;/li&gt;
&lt;li&gt;Include a glob character (&lt;code&gt;*&lt;/code&gt;) in export file name  &lt;ul&gt;
&lt;li&gt;Glob is replaced by shard value of 12 digits  &lt;/li&gt;
&lt;li&gt;Starts with 000000000000 and increments by 1 for each file  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Wildcard exports are written in parallel  &lt;ul&gt;
&lt;li&gt;Target files are smaller and parallel writers work on separate patterns immediately  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Wildcard exports are subject to quota limitations  &lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Single &amp;amp; Multiple Wildcard URI&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Single Wildcard URI  &lt;ul&gt;
&lt;li&gt;&lt;code&gt;'destinationUris': ['gs://my-bucket/file-name-*.json']&lt;/code&gt;  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Multiple Wildcard URI  &lt;ul&gt;
&lt;li&gt;&lt;code&gt;'destinationUris': ['gs://my-bucket/file-name-1-*.json', 'gs://my-bucket/file-name-2-*.json']&lt;/code&gt;  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;code&gt;destinationUris&lt;/code&gt; property indicates export location(s) and file name(s)  &lt;/li&gt;
&lt;li&gt;Data sharded into multiple files based on the pattern  &lt;/li&gt;
&lt;/ul&gt;
&lt;hr /&gt;
&lt;h1&gt;Module 12: Interfacing with External Tools&lt;/h1&gt;
&lt;h2&gt;Interfacing with Spreadsheets&lt;/h2&gt;
&lt;h3&gt;BigQuery Connector for Microsoft Excel&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Supports Excel 2007 and up  &lt;/li&gt;
&lt;li&gt;Supports Windows and Mac  &lt;/li&gt;
&lt;li&gt;Access through authorization key  &lt;ul&gt;
&lt;li&gt;Time sensitive  &lt;/li&gt;
&lt;li&gt;Min 1 hour – Max 30 days  &lt;/li&gt;
&lt;li&gt;Key can be revoked  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Go to https://bigquery-connector.appspot.com  &lt;/li&gt;
&lt;li&gt;Select Google account to use  &lt;/li&gt;
&lt;li&gt;Record unique key and download IQY file  &lt;/li&gt;
&lt;li&gt;Follow site instructions to execute query from Excel  &lt;/li&gt;
&lt;li&gt;&lt;a href="https://cloud.google.com/bigquery/docs/bigquery-connector-for-excel"&gt;https://cloud.google.com/bigquery/docs/bigquery-connector-for-excel&lt;/a&gt;  &lt;ul&gt;
&lt;li&gt;&lt;a href="https://bigquery-connector.appspot.com"&gt;https://bigquery-connector.appspot.com&lt;/a&gt;  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Using Google Sheets with BigQuery&lt;/h3&gt;
&lt;p&gt;Although spreadsheet are not designed to handle big data, many business run on them and use them daily.&lt;br /&gt;
Spreadsheets are understandable by both technical and non-technical staff.&lt;br /&gt;
Spreadsheets allow for use of simple charts and graphs to be easily built.&lt;br /&gt;
You could also connect to BigQuery from Excel via an ODBC connector.  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Extend Google Sheets using App Script  &lt;/li&gt;
&lt;li&gt;Rich interface  &lt;/li&gt;
&lt;li&gt;JavaScript-based language  &lt;ul&gt;
&lt;li&gt;Create buttons, pulldowns, and so on  &lt;/li&gt;
&lt;li&gt;Create dynamic query parameters  &lt;/li&gt;
&lt;li&gt;Create visualizations  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;OWOX BigQuery Reports Add-On  &lt;ul&gt;
&lt;li&gt;Save queries with preset variables  &lt;/li&gt;
&lt;li&gt;Create visualizations  &lt;/li&gt;
&lt;li&gt;Share results  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Alternative to writing scripts  &lt;/li&gt;
&lt;li&gt;Free version available  &lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;ODBC &amp;amp; JDBC Drivers&lt;/h2&gt;
&lt;h3&gt;Simba ODBC/JDBC Drivers (Beta)&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Simba ODBC/JDBC Drivers 32-bit and 64-bit Available for Mac, Linux, Windows  &lt;/li&gt;
&lt;li&gt;Supports ANSI SQL-92: SELECT, JOIN, WHERE, HAVING, GROUP  &lt;/li&gt;
&lt;li&gt;BY, ORDER BY, TOP and most SQL-92 scalar and aggregate functions  &lt;/li&gt;
&lt;li&gt;Supports BigQuery’s SQL subset: SELECT, HAVING, WHERE, GROUP BY, ORDER BY, LIMIT, CASE and all functions  &lt;/li&gt;
&lt;li&gt;Supports all BigQuery data types (STRING, INTEGER, FLOAT, BOOLEAN, TIMESTAMP)  &lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Google has partnered with Simba Technologies to provide updated ODBC and JDBC drivers that leverage the power of BigQuery's Standard SQL (support is also provided for legacy SQL).&lt;br /&gt;
For more information on the Simba ODBC/JDBC drivers for BigQuery, see: &lt;a href="https://cloud.google.com/bigquery/partners/simba-beta-drivers"&gt;https://cloud.google.com/bigquery/partners/simba-beta-drivers&lt;/a&gt;.  &lt;/p&gt;
&lt;h3&gt;Other JDBC Drivers&lt;/h3&gt;
&lt;p&gt;還不 support insert 和 delete  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Starschema JDBC driver for BigQuery  &lt;ul&gt;
&lt;li&gt;Supports server and OAuth2 authentication  &lt;/li&gt;
&lt;li&gt;Supports handling metadata  &lt;/li&gt;
&lt;li&gt;Query transformation capabilities  &lt;/li&gt;
&lt;li&gt;Released to open source - No longer under active development  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;CData JDBC driver for BigQuery  &lt;ul&gt;
&lt;li&gt;&lt;a href="http://www.cdata.com/drivers/bigquery/"&gt;http://www.cdata.com/drivers/bigquery/&lt;/a&gt;  &lt;/li&gt;
&lt;li&gt;Abstracts BigQuery data source into tables, views, stored procedures use to access data  &lt;/li&gt;
&lt;li&gt;要用的時候要跟 Google 拿金鑰放在 JDBS 的 Driver 裏面  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Although the Starschema driver is available, no active work has been done since June 2013.&lt;br /&gt;
This means that any enhancements to BigQuery may not be reflected in the driver.  &lt;/p&gt;
&lt;h2&gt;Encrypted BigQuery Client&lt;/h2&gt;
&lt;h3&gt;Encrypted Client&lt;/h3&gt;
&lt;p&gt;&lt;code&gt;ebq&lt;/code&gt; commnad&lt;br /&gt;
&lt;a href="https://github.com/google/encrypted-bigquery-client"&gt;https://github.com/google/encrypted-bigquery-client&lt;/a&gt;&lt;br /&gt;
&lt;a href="https://github.com/google/encrypted-bigquery-client/blob/master/tutorial.md"&gt;https://github.com/google/encrypted-bigquery-client/blob/master/tutorial.md&lt;/a&gt;  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;An experimental extension to the BigQuery client  &lt;/li&gt;
&lt;li&gt;Offers client-side encryption for a subset of query types  &lt;/li&gt;
&lt;li&gt;Implemented in Python  &lt;/li&gt;
&lt;li&gt;Encrypts data before loading and transforms query to work on top of encrypted data  &lt;/li&gt;
&lt;li&gt;Only available as a replacement for bq CLI  &lt;/li&gt;
&lt;li&gt;Supports multiple encryption modes  &lt;ul&gt;
&lt;li&gt;&lt;code&gt;Pseudonym&lt;/code&gt;  &lt;ul&gt;
&lt;li&gt;encrypts the data the same way, given a particular key  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Probabilistic&lt;/code&gt;  &lt;ul&gt;
&lt;li&gt;encrypts the same text differently every time  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Homomorphic&lt;/code&gt;  &lt;ul&gt;
&lt;li&gt;Encrypts numeric fields with special mathematical properties allowing mathematical operations on encrypted data to yield encrypted results  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Searchwords&lt;/code&gt;  &lt;ul&gt;
&lt;li&gt;Encrypts data so you can find a particular word within a longer string  &lt;/li&gt;
&lt;li&gt;same word is encrypted the same way every time  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Probabilistic_searchwords&lt;/code&gt;  &lt;ul&gt;
&lt;li&gt;combines the two types of encryption so that a word in encrypted a different way every time  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;code&gt;None&lt;/code&gt;  &lt;ul&gt;
&lt;li&gt;No encryption  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Client Interaction&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Normal client interaction  &lt;ul&gt;
&lt;li&gt;Data on client in normal (unencrypted) state  &lt;/li&gt;
&lt;li&gt;Data moves between client and BigQuery over SSH  &lt;/li&gt;
&lt;li&gt;Data is encrypted in flight and at rest once in BigQuery  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Encrypted client interaction  &lt;ul&gt;
&lt;li&gt;Interface encrypts input data on client  &lt;/li&gt;
&lt;li&gt;Encrypted data moves between client and BigQuery over SSH  &lt;/li&gt;
&lt;li&gt;Data encrypted in flight and at rest once in BigQuery  &lt;/li&gt;
&lt;li&gt;Query results to client are encrypted  &lt;/li&gt;
&lt;li&gt;Client interface decrypts the results  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;BigQuery and R&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Environment for statistical computing  &lt;/li&gt;
&lt;li&gt;Contains large, integrated collection of data analysis tools  &lt;/li&gt;
&lt;li&gt;Graphical facilities  &lt;/li&gt;
&lt;li&gt;Simple and effective programming language  &lt;/li&gt;
&lt;li&gt;BigQuery added as extension package  &lt;/li&gt;
&lt;li&gt;BigQuery allows R to process very large datasets  &lt;/li&gt;
&lt;li&gt;Hundreds of modeling packages available  &lt;/li&gt;
&lt;li&gt;R provides very sophisticated analysis  &lt;/li&gt;
&lt;li&gt;Easy setup and use  &lt;/li&gt;
&lt;/ul&gt;
&lt;hr /&gt;
&lt;h1&gt;Module 13: Working with Google Analytics Premium Data&lt;/h1&gt;
&lt;hr /&gt;
&lt;h1&gt;Module 14: Data Visualization&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.google.com/analytics/data-studio/"&gt;https://www.google.com/analytics/data-studio/&lt;/a&gt;  &lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Third-party tools&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;BigQuery integrates with several open-source/commercial tools  &lt;ul&gt;
&lt;li&gt;Tableau, Qlik, iCharts  &lt;/li&gt;
&lt;li&gt;See &lt;a href="https://cloud.google.com/bigquery/third-party-tools"&gt;third-party tools and services&lt;/a&gt;  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;All tools provide report, dashboard creation capability  &lt;/li&gt;
&lt;li&gt;Vendor offerings may be cloud-based, client-based, both  &lt;/li&gt;
&lt;li&gt;Each tool may have a different underlying proprietary technology  &lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Spreadsheet Visualization&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Cost-effective option  &lt;/li&gt;
&lt;li&gt;Limited in business intelligence functionality  &lt;/li&gt;
&lt;li&gt;Visualization capabilities may not be as robust as a business intelligence tool  &lt;ul&gt;
&lt;li&gt;May require additional scripting  &lt;/li&gt;
&lt;li&gt;搭配 Google App Script 使用  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Use third-party application or ODBC driver  &lt;ul&gt;
&lt;li&gt;BigQuery Connector for Microsoft Excel  &lt;/li&gt;
&lt;li&gt;Simba ODBC Driver  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Datalab&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://cloud.google.com/datalab/docs/quickstarts/"&gt;https://cloud.google.com/datalab/docs/quickstarts/&lt;/a&gt;  &lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Use Datalab on Google Cloud Shell&lt;/h3&gt;
&lt;p&gt;好像只要直接在 Cloud Shell 用&lt;a href="https://cloud.google.com/datalab/docs/quickstarts/quickstart-local"&gt;在 local 用 docker 執行 datalab 的方法&lt;/a&gt;就行了&lt;br /&gt;
&lt;code&gt;docker run -it -p "127.0.0.1:8081:8080" -v "${HOME}:/content" -e "PROJECT_ID=${PROJECT_ID}" gcr.io/cloud-datalab/datalab:local&lt;/code&gt;  &lt;/p&gt;
&lt;p&gt;不需要用到 &lt;a href="https://cloud.google.com/datalab/docs/quickstarts/quickstart-gcp"&gt;https://cloud.google.com/datalab/docs/quickstarts/quickstart-gcp&lt;/a&gt;&lt;br /&gt;
&lt;code&gt;docker run -it -p "127.0.0.1:8081:8080" -v "${HOME}:/content" -e "GATEWAY_VM=project-id/zone/instance-name" gcr.io/cloud-datalab/datalab:local&lt;/code&gt;&lt;br /&gt;
因為用這個方法似乎還得額外開一台 VM。  &lt;/p&gt;
&lt;p&gt;在 Cloud Shell 輸入&lt;br /&gt;
&lt;code&gt;docker run -it -p "127.0.0.1:8081:8080" -v "${HOME}:/content" -e "PROJECT_ID=${DEVSHELL_PROJECT_ID}" gcr.io/cloud-datalab/datalab:local&lt;/code&gt;&lt;br /&gt;
(好像有些 Cloud Shell 不知道為何會沒有 &lt;code&gt;$DEVSHELL_PROJECT_ID&lt;/code&gt;，沒有的話就手動輸入吧)&lt;br /&gt;
好了之後再點選左上角第一個 Web preview，選擇 Change port 8081，應該就會開啟一個連到剛剛建立的 datalab 的分頁了  &lt;/p&gt;
&lt;hr /&gt;
&lt;h1&gt;BigQuery Public datasets&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://www.gdeltproject.org/data.html#googlebigquery"&gt;http://www.gdeltproject.org/data.html#googlebigquery&lt;/a&gt;  &lt;ul&gt;
&lt;li&gt;&lt;a href="http://nyctaximap.appspot.com/"&gt;http://nyctaximap.appspot.com/&lt;/a&gt;  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.reddit.com/r/bigquery/"&gt;https://www.reddit.com/r/bigquery/&lt;/a&gt;  &lt;/li&gt;
&lt;li&gt;&lt;a href="https://cloud.google.com/bigquery/public-data/"&gt;https://cloud.google.com/bigquery/public-data/&lt;/a&gt;  &lt;/li&gt;
&lt;/ul&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">m157q</dc:creator><pubDate>Fri, 16 Dec 2016 17:07:43 +0800</pubDate><guid isPermaLink="false">tag:blog.m157q.tw,2016-12-15:posts/2016/12/15/cpb200-bigquery-for-data-analysts/</guid><category>Google Cloud Platform</category></item><item><title>Taiwan Customer Sharing Session - with Google Big Data Engineering team</title><link>https://blog.m157q.tw/posts/2016/06/03/taiwan-customer-sharing-session-with-google-big-data-engineering-team/</link><description>&lt;h1&gt;Big Data at Google&lt;/h1&gt;
&lt;h2&gt;Speaker: Apurva Desai&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;Apurva Desai has over 20 years of experience in building software and managing teams.  He has been with early stage startups to big sized companies.   He has been focused on cloud, big data and distributed computing for the last 10+years starting with Yahoo where his team was responsible to manage 20k+ nodes of Hadoop and provide solutions to internal projects migrating to Hadoop ecosystem.  At Pivotal Software, a spinoff of EMC, his team built and commercialized Pivotal’s Hadoop distribution.  Most recently he managed Motorola’s mobile phone experiences powered by mobile cloud backend running on GCP.  Apurva earned his Bachelor’s of engineering from University of Mumbai, India and Master of engineering from Simon Fraser University, Canada  &lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://research.google.com/pubs/pub41378.html"&gt;MillWheel&lt;/a&gt;  &lt;/li&gt;
&lt;li&gt;&lt;a href="https://flume.apache.org/"&gt;Flume&lt;/a&gt;  &lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;What does Cloud 3.0 look like?&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Cloud 2.0 (Assembly Required)  &lt;ul&gt;
&lt;li&gt;VMs  &lt;/li&gt;
&lt;li&gt;Object Store  &lt;/li&gt;
&lt;li&gt;Databases  &lt;/li&gt;
&lt;li&gt;Networking  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Cloud 3.0  &lt;ul&gt;
&lt;li&gt;Containers  &lt;/li&gt;
&lt;li&gt;Messaging  &lt;/li&gt;
&lt;li&gt;NoSQL  &lt;/li&gt;
&lt;li&gt;Big Data  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Complixities of Big Data Processing&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Programming  &lt;/li&gt;
&lt;li&gt;Resource provisioning  &lt;/li&gt;
&lt;li&gt;Handling growing scale  &lt;/li&gt;
&lt;li&gt;Reliability  &lt;/li&gt;
&lt;li&gt;Deployment &amp;amp; Configuration  &lt;/li&gt;
&lt;li&gt;Utilization improvements  &lt;/li&gt;
&lt;li&gt;Performance tuning  &lt;/li&gt;
&lt;li&gt;Monitoring  &lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;But what you should do is focusing on programming and dig your data.  &lt;/p&gt;
&lt;h2&gt;10+ Years of Tackling Big Data Problems&lt;/h2&gt;
&lt;p&gt;&lt;img alt="10+ Years of Tackling Big Data Problem" src="/files/taiwan-customer-sharing-session-with-google-big-data-engineering-team/google-big-data-history.jpg" /&gt;  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://en.wikipedia.org/wiki/Google_File_System"&gt;GFS&lt;/a&gt; (2002 ~ 2004)  &lt;/li&gt;
&lt;li&gt;MapReduce (2004 ~ 2005)  &lt;ul&gt;
&lt;li&gt;Dataflow (GCP)  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;BigTable (2005 ~ 2006)  &lt;ul&gt;
&lt;li&gt;Apache HBase  &lt;/li&gt;
&lt;li&gt;Hadoop  &lt;/li&gt;
&lt;li&gt;Bigtable (GCP)  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Dremel (2006 ~ 2008)  &lt;ul&gt;
&lt;li&gt;Apache Drill  &lt;/li&gt;
&lt;li&gt;BigQuery (GCP)  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;PubSub (2008 ~ 2010)  &lt;ul&gt;
&lt;li&gt;Pub/Sub (GCP)  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;FlumeJava (2010 ~ 2012)  &lt;ul&gt;
&lt;li&gt;Apache Crunch  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;MillWheel (2012 ~ 2014)  &lt;ul&gt;
&lt;li&gt;&lt;a href="http://beam.apache.org"&gt;Apache Beam&lt;/a&gt;  &lt;ul&gt;
&lt;li&gt;Dataflow (GCP)  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;TensorFlow (2014 ~ now)  &lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;"Google is living a few years in the future and sending the rest of us messages." - Doug Cutting, Hadoop Co-Creator  &lt;/p&gt;
&lt;h2&gt;Bridging the Waves&lt;/h2&gt;
&lt;p&gt;&lt;img alt="Bridging the Waves" src="/files/taiwan-customer-sharing-session-with-google-big-data-engineering-team/bridging-the-waves.jpg" /&gt;  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Capture  &lt;ul&gt;
&lt;li&gt;Cloud 3.0  &lt;ul&gt;
&lt;li&gt;Pub/Sub  &lt;/li&gt;
&lt;li&gt;Logs, App Engine  &lt;/li&gt;
&lt;li&gt;BigQuery streaming  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Cloud 2.0  &lt;ul&gt;
&lt;li&gt;Rabbit MQ  &lt;/li&gt;
&lt;li&gt;Kafka  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Store  &lt;ul&gt;
&lt;li&gt;Cloud 3.0  &lt;ul&gt;
&lt;li&gt;Cloud Storage (objects)  &lt;/li&gt;
&lt;li&gt;BigQuery Storage (structured)  &lt;/li&gt;
&lt;li&gt;Cloud Bigtable (NoSQL HBase)  &lt;/li&gt;
&lt;li&gt;Cloud Datastore (NoSQL)  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Cloud 2.0  &lt;ul&gt;
&lt;li&gt;Cassandra  &lt;/li&gt;
&lt;li&gt;HBase  &lt;/li&gt;
&lt;li&gt;MongoDB  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Process  &lt;ul&gt;
&lt;li&gt;Cloud 3.0  &lt;ul&gt;
&lt;li&gt;Cloud Dataflow (stream and batch)  &lt;/li&gt;
&lt;li&gt;Cloud Dataproc  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Cloud 2.0  &lt;ul&gt;
&lt;li&gt;Hadoop &amp;amp; Ecosystem  &lt;ul&gt;
&lt;li&gt;Spark  &lt;/li&gt;
&lt;li&gt;Hive  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Analyze  &lt;ul&gt;
&lt;li&gt;Cloud 3.0  &lt;ul&gt;
&lt;li&gt;BigQuery (large scale SQL)  &lt;/li&gt;
&lt;li&gt;Cloud Machine Learning  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Cloud 2.0  &lt;ul&gt;
&lt;li&gt;Hadoop &amp;amp; Ecosystem  &lt;ul&gt;
&lt;li&gt;Spark  &lt;/li&gt;
&lt;li&gt;Hive  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Visualize  &lt;ul&gt;
&lt;li&gt;Cloud 3.0  &lt;ul&gt;
&lt;li&gt;Cloud DataLab (Python/Jupyter Notebook)  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Cloud 2.0  &lt;ul&gt;
&lt;li&gt;Tableau  &lt;/li&gt;
&lt;li&gt;Qlik  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Reference Architecture&lt;/h2&gt;
&lt;p&gt;&lt;img alt="Reference Architecture" src="/files/taiwan-customer-sharing-session-with-google-big-data-engineering-team/reference-architecture.jpg" /&gt;  &lt;/p&gt;
&lt;h2&gt;Apache Beam and Google Cloud Dataflow&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Dataflow  &lt;ul&gt;
&lt;li&gt;API Interface (SDK)  &lt;ul&gt;
&lt;li&gt;Dataflow model / Beam model  &lt;/li&gt;
&lt;li&gt;You can write you own sdk with any language  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Apache Beam  &lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Beam vs Spark&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Classic Batch Processing  &lt;ul&gt;
&lt;li&gt;Simillar  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Windowing  &lt;ul&gt;
&lt;li&gt;Beam  &lt;ul&gt;
&lt;li&gt;Window  &lt;/li&gt;
&lt;li&gt;Sum  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Spark  &lt;ul&gt;
&lt;li&gt;Window &amp;amp; Sum  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Late Data  &lt;ul&gt;
&lt;li&gt;Beam is more easy to implement in this part  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Sessions  &lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Google Cloud Dataflow&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Fully-managed and auto-configured  &lt;/li&gt;
&lt;li&gt;Auto graph-optimized for best execution path  &lt;/li&gt;
&lt;li&gt;Autoscaling mid-job  &lt;/li&gt;
&lt;li&gt;Dynamic Work Rebalancing mid-job  &lt;/li&gt;
&lt;li&gt;Fault Tolerant execution of Beam pipelines  &lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Dataproc - Managed Hadoop + Spark&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;Start a massive Hadoop or Spark cluster in 90 seconds  &lt;/li&gt;
&lt;li&gt;Pre emptible VMs at 30% of othe cost, Custom VMs  &lt;/li&gt;
&lt;li&gt;Per-minute billing  &lt;/li&gt;
&lt;li&gt;Separation of Storage + Compute  &lt;/li&gt;
&lt;li&gt;Incredibly fast networking  &lt;/li&gt;
&lt;/ol&gt;
&lt;h3&gt;Ephemeral clusters - jobs before clusters&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Deploy Cluster  &lt;/li&gt;
&lt;li&gt;Submits jobs  &lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Separation of Storage and Compute&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Based on your use cases  &lt;/li&gt;
&lt;/ul&gt;
&lt;hr /&gt;
&lt;h2&gt;BigQuery&lt;/h2&gt;
&lt;h3&gt;Fun BigQuery Stats&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Largest query by rows =&amp;gt; 10.5 Trillion rows  &lt;/li&gt;
&lt;li&gt;Larget query by data size =&amp;gt; 2.1 PB  &lt;/li&gt;
&lt;li&gt;Largest storage customer =&amp;gt; 62 PB  &lt;/li&gt;
&lt;li&gt;Streaming per second =&amp;gt; 4.5 million  &lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;BigQuery - explained&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;We just rented ~9000 cores from Google for ~30 seconds  &lt;/li&gt;
&lt;li&gt;We only paid $20  &lt;/li&gt;
&lt;li&gt;Most importantly, it's hide from end users.  &lt;/li&gt;
&lt;li&gt;Users do not thins about cores.  &lt;/li&gt;
&lt;/ol&gt;
&lt;h3&gt;What is BigQuery?&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Serveless, Fully Managed, No-Ops Data Warehouse  &lt;/li&gt;
&lt;li&gt;Petabyte-Scale and Fast  &lt;/li&gt;
&lt;li&gt;Convenience of SQL  &lt;/li&gt;
&lt;li&gt;Externalization of Google Dremel  &lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Cloud Pub/Sub - Asynchronous Messaging&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;A FULLY-MANAGED GLOBAL Publish and Subscribe service (a many-to-many queue)  &lt;/li&gt;
&lt;li&gt;Seamlessly scales to 1,000,000+ QPS  &lt;/li&gt;
&lt;li&gt;Guaranteed durable at-least-once delivery  &lt;/li&gt;
&lt;li&gt;7-day message acknowledgement window  &lt;/li&gt;
&lt;li&gt;Simple REST API makes it portable  &lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;References&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.oreilly.com/ideas/the-world-beyond-batch-streaming-102"&gt;The world beyond batch: Streaming 102 - O'Reilly Media&lt;/a&gt;  &lt;/li&gt;
&lt;/ul&gt;
&lt;hr /&gt;
&lt;h1&gt;Case study: Spotify's Event Delivery System&lt;/h1&gt;
&lt;h2&gt;Speaker: Jelena Pješivac-Grbović&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;Dr. Jelena Pješivac-Grbović is a staff software engineer in Cloud at Google, in Mountain View, CA. She is the lead for MapReduce and one of the leads of the Google Cloud Dataflow project.  Jelena's research interests include large-scale data processing, distributed, and cloud computing. She is an active member of IEEE, ACM, and SWE.  &lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;其實就是把 References 那三篇稍微帶過這樣。  &lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;Event Delivery System  &lt;ul&gt;
&lt;li&gt;High QPS  &lt;ul&gt;
&lt;li&gt;~700K events/sec in peak  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;References&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://labs.spotify.com/2016/02/25/spotifys-event-delivery-the-road-to-the-cloud-part-i/"&gt;Spotify’s Event Delivery – The Road to the Cloud (Part I) | Labs&lt;/a&gt;  &lt;/li&gt;
&lt;li&gt;&lt;a href="https://labs.spotify.com/2016/03/03/spotifys-event-delivery-the-road-to-the-cloud-part-ii/"&gt;Spotify’s Event Delivery – The Road to the Cloud (Part II) | Labs&lt;/a&gt;  &lt;/li&gt;
&lt;li&gt;&lt;a href="https://labs.spotify.com/2016/03/10/spotifys-event-delivery-the-road-to-the-cloud-part-iii/"&gt;Spotify’s Event Delivery – The Road to the Cloud (Part III) | Labs&lt;/a&gt;  &lt;/li&gt;
&lt;/ul&gt;
&lt;hr /&gt;
&lt;p&gt;附上一張到此一遊照 (?)  &lt;/p&gt;
&lt;p&gt;&lt;img alt="GCP VIP Customer Workshop" src="/files/taiwan-customer-sharing-session-with-google-big-data-engineering-team/gcp-vip-customer-workshop.jpg" /&gt;  &lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">m157q</dc:creator><pubDate>Fri, 03 Jun 2016 10:15:32 +0800</pubDate><guid isPermaLink="false">tag:blog.m157q.tw,2016-06-03:posts/2016/06/03/taiwan-customer-sharing-session-with-google-big-data-engineering-team/</guid><category>Google Cloud Platform</category></item><item><title>CP100A: Google Cloud Platform Fundamentals</title><link>https://blog.m157q.tw/posts/2016/05/25/cp100a-google-cloud-platform-fundamentals/</link><description>&lt;h1&gt;課程資訊&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://myclass.gcptrain.org/"&gt;http://myclass.gcptrain.org/&lt;/a&gt;  &lt;ul&gt;
&lt;li&gt;OXZnOGVkCg==  &lt;/li&gt;
&lt;li&gt;&lt;a href="https://sites.google.com/a/google.com/cloud-platform-training/cloud-platform-training/"&gt;https://sites.google.com/a/google.com/cloud-platform-training/cloud-platform-training/&lt;/a&gt;  &lt;/li&gt;
&lt;li&gt;&lt;a href="https://sites.google.com/a/google.com/cloud-platform-training/cloud-platform-training/cp100-v2"&gt;https://sites.google.com/a/google.com/cloud-platform-training/cloud-platform-training/cp100-v2&lt;/a&gt;  &lt;/li&gt;
&lt;li&gt;&lt;a href="https://drive.google.com/file/d/0B9cCeTKOkfWIaXNqWnNDT0VmaG8/view?usp=sharing"&gt;slides.tar.gz&lt;/a&gt;  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;hr /&gt;
&lt;h1&gt;Course Overview&lt;/h1&gt;
&lt;p&gt;&lt;img alt="CP100 V2: Google Cloud Platform Fundamentals" src="/files/cp100a-google-cloud-platform-fundamentals/course-overview.png" /&gt;  &lt;/p&gt;
&lt;hr /&gt;
&lt;h1&gt;Module 1: Introducing Google Cloud Platform&lt;/h1&gt;
&lt;h2&gt;Why Choose Google Cloud Platform?&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;你可以在 GCP 看到所有不同 Region 的機器，不用像 AWS 一樣必須切換 Region  &lt;/li&gt;
&lt;li&gt;可以直接享用 Google 遍布全球的網路設施  &lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Google's Infrastructure&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://cloudplatformonline.com/next2016-schedule.html"&gt;GCP Next&lt;/a&gt;  &lt;ul&gt;
&lt;li&gt;GCP 的年度會議  &lt;blockquote&gt;
&lt;p&gt;目前似乎辦了兩屆。&lt;br /&gt;
2015 年第一屆辦在日本東京&lt;br /&gt;
2016 年第二屆辦在荷蘭阿姆斯特丹。  &lt;/p&gt;
&lt;/blockquote&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;最近在日本新增了 Data Center  &lt;/li&gt;
&lt;li&gt;Google 的高速 Backbone Network  &lt;/li&gt;
&lt;li&gt;Points of Presence  &lt;ul&gt;
&lt;li&gt;幾乎全球都有節點  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Edge Caching  &lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Cloud Regions and Zones&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Central US  &lt;/li&gt;
&lt;li&gt;Eastern US  &lt;/li&gt;
&lt;li&gt;East Asia  &lt;ul&gt;
&lt;li&gt;Data Center 在彰化  &lt;/li&gt;
&lt;li&gt;和 CloudFlare 有合作，最近 CloudFlare 和中華電信合作，在台北有機房。  &lt;/li&gt;
&lt;li&gt;所以在臺灣的 latency 蠻低的  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Western Europe  &lt;/li&gt;
&lt;li&gt;&lt;a href="https://cloud.google.com/compute/docs/regions-zones/regions-zones"&gt;https://cloud.google.com/compute/docs/regions-zones/regions-zones&lt;/a&gt;  &lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Innovative, Customer-Friendly Pricing&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Sub-hour billing  &lt;ul&gt;
&lt;li&gt;以分計費  &lt;/li&gt;
&lt;li&gt;不像 AWS 以小時計費，不滿一小時仍然以一小時計費  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Sustained-use discounts  &lt;ul&gt;
&lt;li&gt;機器開超過一定的時間就會有折扣，採累進的折扣。  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Compute Engine custom machine types  &lt;/li&gt;
&lt;li&gt;價錢比較便宜，但有夠難算 XDDD  &lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Commitment to Open APIs and Open Source&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;TensorFlow  &lt;/li&gt;
&lt;li&gt;Android  &lt;/li&gt;
&lt;li&gt;Go  &lt;/li&gt;
&lt;li&gt;&lt;a href="http://kubernetes.io/"&gt;Kubernetes&lt;/a&gt; (k8s)  &lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;The Future of Cloud Computing&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;1st wave: Colocation  &lt;/li&gt;
&lt;li&gt;2nd wave: Virtualized Data Centers  &lt;/li&gt;
&lt;li&gt;3rd wave: A global, elastic cloud  &lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;IaaS and PaaS&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;IaaS: Compute Engine  &lt;ul&gt;
&lt;li&gt;Towards managed infrastructure (DevOps)  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;PaaS: App Engine  &lt;ul&gt;
&lt;li&gt;Towards managed services (NoOps)  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Google Cloud Platform&lt;/h2&gt;
&lt;p&gt;&lt;img alt="Google Cloud Platform" src="/files/cp100a-google-cloud-platform-fundamentals/gcp.png" /&gt;  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Storage  &lt;ul&gt;
&lt;li&gt;BigTable  &lt;ul&gt;
&lt;li&gt;Fully Compatible with HBase  &lt;/li&gt;
&lt;li&gt;Google 版本的 HBase  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Cloud SQL  &lt;ul&gt;
&lt;li&gt;最近出了 2.0 (2nd Generation)  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Big Data  &lt;ul&gt;
&lt;li&gt;Pub/Sub  &lt;ul&gt;
&lt;li&gt;Distributed Message Queue like Kafka  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="https://cloud.google.com/dataflow/"&gt;Dataflow&lt;/a&gt;  &lt;ul&gt;
&lt;li&gt;a unified programming model and a managed service for developing and executing a wide range of data processing patterns including ETL, batch computation, and continuous computation.  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="https://cloud.google.com/dataproc/"&gt;Dataproc&lt;/a&gt;  &lt;ul&gt;
&lt;li&gt;Spark Cluster  &lt;/li&gt;
&lt;li&gt;an Apache Hadoop, Apache Spark, Apache Pig, and Apache Hive service, to easily process big datasets at low cost.  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Datalab  &lt;ul&gt;
&lt;li&gt;基本上就是 Google Cloud 版本的 Jupyter Notebook (IPython Notebook)  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Google Cloud Launcher&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;和 Bitnami 合作提供的服務  &lt;/li&gt;
&lt;li&gt;可以直接在上面直接 Create 設定好的 GCE instance  &lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Lab 1&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://codelabs.developers.google.com/codelabs/cp100-free-trial/#0"&gt;Sign Up for the Free Trial and Create a Project&lt;/a&gt;  &lt;ul&gt;
&lt;li&gt;&lt;a href="https://cloud.google.com/stackdriver/"&gt;Stackdriver&lt;/a&gt;  &lt;ul&gt;
&lt;li&gt;monitoring, logging, &amp;amp; diagnostics  &lt;/li&gt;
&lt;li&gt;可以整合 GCP 和 AWS  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="https://console.cloud.google.com/launcher"&gt;Cloud Launcher&lt;/a&gt;  &lt;ul&gt;
&lt;li&gt;&lt;a href="https://cloud.google.com/launcher/docs/#deploying_a_software_package"&gt;https://cloud.google.com/launcher/docs/#deploying_a_software_package&lt;/a&gt;  &lt;/li&gt;
&lt;li&gt;點下去可以直接幫你 create 安裝好該服務的 GCE instance，簡單來說就是已經預先做好 Image 然後直接幫你塞進去。  &lt;ul&gt;
&lt;li&gt;我原本以為是可以複選，然後一次幫你安裝剛剛選的那些服務到一台 GCE instance 上，但看來是比較提倡分散式就是了，當然這樣在 Production 上會比較好，不然一台 instance 炸了就所有服務都炸了 XD  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;補充&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Project 的管理  &lt;ul&gt;
&lt;li&gt;Members 的 account 可以採用 gmail.com, apps for work 的 account, Google Groups 的 account, service account  &lt;/li&gt;
&lt;li&gt;一個帳號可以管理多個 project  &lt;/li&gt;
&lt;li&gt;管錢的和管 Project 的帳號可以分開設定  &lt;/li&gt;
&lt;li&gt;可以考慮多開不同的 Project，一來是 Quota 的限制比較不會那麼吃緊，二來是 Permission 的設定可以比較不需要那麼費心，如果全部的 Team 都擠在同個 Project 的話，Permission 的設定可能得多費心調整。  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Billing  &lt;ul&gt;
&lt;li&gt;Sustain Pricing 在遇到 billing account change 的時候會重算，所以  &lt;/li&gt;
&lt;li&gt;可以設定 budget，超過的時候會通知，每個服務也都可以設限。  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;hr /&gt;
&lt;h1&gt;Module 2: Getting Started with Google Cloud Platform&lt;/h1&gt;
&lt;h2&gt;Cloud Computing&lt;/h2&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;Compute Engine --- Container Engine --- App Engine --- Cloud Endpoints  
IaaS ------------- Clusters -------- Managed VMs (beta) -------- PaaS  
Configurability DevOps &amp;lt;-----------------------------&amp;gt; Agility NoOps  
&lt;/pre&gt;&lt;/div&gt;


&lt;ul&gt;
&lt;li&gt;IaaS  &lt;ul&gt;
&lt;li&gt;Compute Engine == AWS EC2 == Virtual Machine  &lt;ul&gt;
&lt;li&gt;Raw compute granular control  &lt;/li&gt;
&lt;li&gt;可以使用預先提供好的 Image，也可以自己建好 Image 再上傳來用  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;PaaS  &lt;ul&gt;
&lt;li&gt;App Engine  &lt;ul&gt;
&lt;li&gt;最早出來的時候是只有 Python  &lt;/li&gt;
&lt;li&gt;有漲價過，當時一堆人離開  &lt;/li&gt;
&lt;li&gt;後來又有一些人回來用，支援 Java, Go, PHP, Python  &lt;/li&gt;
&lt;li&gt;最近 Beta 開始支援 Ruby  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Cloud Endpoints  &lt;ul&gt;
&lt;li&gt;Preset run-times  &lt;/li&gt;
&lt;li&gt;Focus app logic  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;SaaS  &lt;/li&gt;
&lt;li&gt;&lt;a href="https://developers.google.com/apis-explorer/#p/"&gt;Google APIs Explorer&lt;/a&gt;  &lt;ul&gt;
&lt;li&gt;只要是 Google 的服務基本上都會有 API  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Lab 2&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://codelabs.developers.google.com/codelabs/cp100-cloud-launcher/#0"&gt;Getting Started with Google Cloud Platform&lt;/a&gt;  &lt;/li&gt;
&lt;/ul&gt;
&lt;hr /&gt;
&lt;h1&gt;Module 3: Google App Engine and Google Cloud Datastore&lt;/h1&gt;
&lt;h2&gt;Google App Engine&lt;/h2&gt;
&lt;h3&gt;What is Google App Engine&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Managed runtimes for specific versions of Java, Python, PHP and Go. (Standard Runtime)  &lt;/li&gt;
&lt;li&gt;Autoscale workloads to meet demand  &lt;ul&gt;
&lt;li&gt;可以透過 app.yaml 去做 autoscale 的設定  &lt;/li&gt;
&lt;li&gt;也可以透過 app.yaml 對 instance class 做設定，預設是用最低階的 F1，可以參考 &lt;a href="https://cloud.google.com/appengine/docs/python/config/appref#scaling_elements"&gt;app.yaml Reference|Python|Google Cloud Platform&lt;/a&gt;  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Free daily quota, usage based pricing  &lt;/li&gt;
&lt;li&gt;Local SDK for development, testing and deployment  &lt;/li&gt;
&lt;li&gt;Need to conform to sandbox constraints  &lt;ul&gt;
&lt;li&gt;No writing to the local filesystem  &lt;/li&gt;
&lt;li&gt;Request timeouts at 60 seconds  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;補充  &lt;ul&gt;
&lt;li&gt;可以透過 version 來控管每個 service (原本叫 module，最近改叫 service 了）  &lt;/li&gt;
&lt;li&gt;可以透過 split traffic 做 A/B testing  &lt;/li&gt;
&lt;li&gt;有類似 rolling update 的機制  &lt;ul&gt;
&lt;li&gt;Deploy 新的 version 後，GAE 會自動幫你把舊版本的 instance 關掉，然後開新的版本的 instance  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;可以讓開發者專注在發開程式，不用費心在建置環境的部份  &lt;/li&gt;
&lt;li&gt;實例：  &lt;ul&gt;
&lt;li&gt;Snapchat  &lt;ul&gt;
&lt;li&gt;用 App Engine  &lt;/li&gt;
&lt;li&gt;只花流量的費用，不存圖片，超省成本。  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;App Engine Standard Environment&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Managed runtimes for specific versions of Java, Python, PHP, Go.  &lt;ul&gt;
&lt;li&gt;目前只支援 Python 2  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Autoscale  &lt;/li&gt;
&lt;li&gt;Free daily quota, usage based pricing.  &lt;/li&gt;
&lt;li&gt;原本 support 一天發 2000 封 email，但現在收回來了，現在要在 GCP 上寄信的話，統一都要使用 &lt;a href="https://sendgrid.com/"&gt;SendGrid&lt;/a&gt;，會有比較嚴格的審核，避免大量的垃圾信件。  &lt;ul&gt;
&lt;li&gt;AWS 也採用 SendGrid，蠻多 Cloud Platform 都把寄信的部份交給它。  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;跟 Google 的很多服務都有滿完整的整合。  &lt;/li&gt;
&lt;li&gt;GAE 的設計理念是服務要愈 light weight 愈好  &lt;/li&gt;
&lt;li&gt;GAE 的內建服務  &lt;ul&gt;
&lt;li&gt;Memcache  &lt;ul&gt;
&lt;li&gt;免費的會有 crash 的風險，不會幫你把 data 復原。  &lt;/li&gt;
&lt;li&gt;付費的會在 crash 的時候幫你把 data 復原。  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Taskqueues  &lt;ul&gt;
&lt;li&gt;用來設計保證該 task 一定會被完成的架構  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Scheduled tasks  &lt;ul&gt;
&lt;li&gt;cron.yaml  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Blobstore  &lt;/li&gt;
&lt;li&gt;Search  &lt;/li&gt;
&lt;li&gt;Logging  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;App Engine Flexible Environment (GAE Managed VM)&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;用 container 來處理  &lt;/li&gt;
&lt;li&gt;沒有 sandbox 的限制  &lt;/li&gt;
&lt;li&gt;可以做到支援 Python 3  &lt;/li&gt;
&lt;li&gt;During beta pricing based on GCE  &lt;/li&gt;
&lt;li&gt;Local Development relies on Docker  &lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;GAE Standard vs Flexible Environment 比較表&lt;/h3&gt;
&lt;p&gt;&lt;img alt="GAE Environments" src="/files/cp100a-google-cloud-platform-fundamentals/gae-environments.png" /&gt;  &lt;/p&gt;
&lt;h1&gt;Google Cloud Endpoints&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;Build your own API running on App Engine Standard  &lt;/li&gt;
&lt;li&gt;Expose your API using a RESTful interface  &lt;/li&gt;
&lt;li&gt;Includes support for OAuth 2.0 authorization  &lt;/li&gt;
&lt;li&gt;Generate client libraries  &lt;/li&gt;
&lt;li&gt;Supports Java and Python server-side code  &lt;/li&gt;
&lt;li&gt;Includes App Engine features  &lt;ul&gt;
&lt;li&gt;Scaling  &lt;/li&gt;
&lt;li&gt;Denial of service protection  &lt;/li&gt;
&lt;li&gt;High availability  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Supports iOS, Android, and JavaScript  &lt;/li&gt;
&lt;li&gt;補充  &lt;ul&gt;
&lt;li&gt;可以自動 generate client library  &lt;/li&gt;
&lt;li&gt;目前 support Java 跟 Python  &lt;/li&gt;
&lt;li&gt;直接 apply GAE 的一些 feature  &lt;/li&gt;
&lt;li&gt;HA  &lt;/li&gt;
&lt;li&gt;Support iOS, Android and JavaScript clients  &lt;/li&gt;
&lt;li&gt;但因為是在 GAE 上在堆疊一層，所以當量很大的時候，效能可能要注意一下  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Google Cloud Datastore&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Daily free quota  &lt;/li&gt;
&lt;li&gt;Database designed for application backends  &lt;/li&gt;
&lt;li&gt;NoSQL store for billions of rows  &lt;/li&gt;
&lt;li&gt;Schemaless access, no need to think about underlying data structure  &lt;/li&gt;
&lt;li&gt;Local development tools  &lt;/li&gt;
&lt;li&gt;Automatic scaling and fully managed  &lt;/li&gt;
&lt;li&gt;Built-in redundancy  &lt;/li&gt;
&lt;li&gt;Supports ACID transactions  &lt;/li&gt;
&lt;li&gt;RESTful API  &lt;/li&gt;
&lt;li&gt;Includes a free daily quota  &lt;/li&gt;
&lt;li&gt;Access from anywhere through a RESTful interface  &lt;/li&gt;
&lt;li&gt;補充  &lt;ul&gt;
&lt;li&gt;有 autoscale 的能力，會對應 GAE 的數量來去調整  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Lab 3&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://codelabs.developers.google.com/codelabs/cp100-app-engine/"&gt;Deploying Applications Using App Engine and Cloud Datastore&lt;/a&gt;  &lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/GoogleCloudPlatformTraining/cp100-bookshelf"&gt;https://github.com/GoogleCloudPlatformTraining/cp100-bookshelf&lt;/a&gt;  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;hr /&gt;
&lt;h1&gt;Module 4: Google Cloud Platform Storage Options&lt;/h1&gt;
&lt;h2&gt;Google Cloud Storage&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Not a file system (but can be accessed as one via 3rd party tools such as GCS Fuse)  &lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/GoogleCloudPlatform/gcsfuse"&gt;https://github.com/GoogleCloudPlatform/gcsfuse&lt;/a&gt;  &lt;/li&gt;
&lt;li&gt;IO 不快  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Simple administration and does not require capacity management  &lt;/li&gt;
&lt;li&gt;All storage options accessed through the same APIs and include client libraries  &lt;ul&gt;
&lt;li&gt;JSON API  &lt;/li&gt;
&lt;li&gt;XML API  &lt;ul&gt;
&lt;li&gt;可能是因為 AWS S3 是用 XML API，所以也要跟著提供一下。  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;補充  &lt;ul&gt;
&lt;li&gt;硬碟上的資料是有做 encryption 的  &lt;/li&gt;
&lt;li&gt;容器是以 bucket 為單位  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Cloud Storage Classes&lt;/h3&gt;
&lt;p&gt;&lt;img alt="Cloud Storage Classes" src="/files/cp100a-google-cloud-platform-fundamentals/cloud-storage-classes.png" /&gt;  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Standard  &lt;/li&gt;
&lt;li&gt;DRA  &lt;ul&gt;
&lt;li&gt;可以限制資料的區域  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Nearline  &lt;ul&gt;
&lt;li&gt;經常變動的資料不適合存在這裡，cost 會增加。  &lt;/li&gt;
&lt;li&gt;比較適合拿來做 backup, archive, 長久性不太會變動的資料。  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;這 3 個 classes 存取的 API 是相同的  &lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Cloud Storage Features&lt;/h3&gt;
&lt;p&gt;&lt;img alt="Cloud Storage Features" src="/files/cp100a-google-cloud-platform-fundamentals/cloud-storage-features.png" /&gt;  &lt;/p&gt;
&lt;h3&gt;Cloud Storage Integration&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;BigQuery  &lt;ul&gt;
&lt;li&gt;Import and export tables  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Compute Engine  &lt;ul&gt;
&lt;li&gt;Startup scripts, images and general object storage  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;App Engine  &lt;ul&gt;
&lt;li&gt;Object storage, logs, Datastore backup  &lt;/li&gt;
&lt;li&gt;App Engine 本身不能存資料，但可以存在 Cloud Storage 和 Datastore  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Cloud SQL  &lt;ul&gt;
&lt;li&gt;Import and export tables  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;可以拿來直接 serve static websites.  &lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Google Cloud Bigtable&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;NoSQL database service for large-workload applications (Terabytes to Petabytes)  &lt;ul&gt;
&lt;li&gt;不便宜  &lt;ul&gt;
&lt;li&gt;貴在 Node 執行時間的收費，目前是 $1.95 USD/hour per node  &lt;/li&gt;
&lt;li&gt;最少必須開 3 個 node  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;是儲存在 SSD 上  &lt;ul&gt;
&lt;li&gt;最近開始可以選擇儲存在普通硬碟上了，Storage 的費用會降低大概十倍。  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Protected  &lt;ul&gt;
&lt;li&gt;Replicated storage  &lt;/li&gt;
&lt;li&gt;Data encryption in-flight and at rest  &lt;/li&gt;
&lt;li&gt;Role-based ACLs  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Proven  &lt;ul&gt;
&lt;li&gt;Gmail and Google Analytics  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;補充  &lt;ul&gt;
&lt;li&gt;高 IO, 可在最短的時間內查到最多的資料  &lt;/li&gt;
&lt;li&gt;Gmail 和 Google Analytics 的背後也是用 Bigtable  &lt;/li&gt;
&lt;li&gt;很多做股票交易的也是用 Bigtable  &lt;/li&gt;
&lt;li&gt;很貴但反應快  &lt;/li&gt;
&lt;li&gt;主要是為了取代 HBase  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Google Cloud SQL&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Google-managed MySQL  &lt;/li&gt;
&lt;li&gt;Pay-per-use model  &lt;/li&gt;
&lt;li&gt;REST API for management  &lt;/li&gt;
&lt;li&gt;Affordability and performance  &lt;ul&gt;
&lt;li&gt;有 class 可以選擇，視需求可以調整  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Google security  &lt;/li&gt;
&lt;li&gt;Vertical scaling (read and write)  &lt;/li&gt;
&lt;li&gt;Horizontal scaling (read)  &lt;/li&gt;
&lt;li&gt;Seamless integratin with GAE and GCE  &lt;/li&gt;
&lt;li&gt;補充  &lt;ul&gt;
&lt;li&gt;第一代的 performance 不是那麼好  &lt;/li&gt;
&lt;li&gt;第二代則是選擇 run 在 container 上  &lt;/li&gt;
&lt;li&gt;所有要連線來的 IP 都需要經過 white list  &lt;ul&gt;
&lt;li&gt;有個例外是 App Engine，可以直接連線，不會被白名單限制。  &lt;/li&gt;
&lt;li&gt;可以設定讓 Cloud SQL 綁定 GAE，讓它開在跟 GAE 同個 region，用來降低 Latency  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;七天一個 cycle 的 backup  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Cloud SQL Features&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Familiar with MySQL  &lt;/li&gt;
&lt;li&gt;Flexible pricing  &lt;/li&gt;
&lt;li&gt;Google Security  &lt;ul&gt;
&lt;li&gt;&lt;a href="https://cloud.google.com/sql/faq#encryption_manage"&gt;AES-128 encryption&lt;/a&gt;  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Managed backups  &lt;/li&gt;
&lt;li&gt;Automatic replication  &lt;ul&gt;
&lt;li&gt;master-slave  &lt;/li&gt;
&lt;li&gt;自動化 replication  &lt;/li&gt;
&lt;li&gt;一個 instance 掛掉的話，會有 downtime 但會再開另外一個 instance 去接替，有基本的 HA 功能。  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;支援 SSL 的 connection  &lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Cloud SQL Second Generation&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Same features as first generation with higher performance, storage capacity at lower cost.  &lt;ul&gt;
&lt;li&gt;Up to 7X throughput and 20X sotrage capacity of first generation instances  &lt;/li&gt;
&lt;li&gt;Less expensive than first generation for most use cases.  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;補充  &lt;ul&gt;
&lt;li&gt;如果想要開比較小的 DB 的話可以考慮用 2nd generation，性價比會比較高。  &lt;/li&gt;
&lt;li&gt;如果是要用很大的 DB 的話，建議用 1st generation 讓 Google 幫忙管理會比較好。  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Comparing Storage Options&lt;/h3&gt;
&lt;p&gt;&lt;img alt="Comparing Storage Options" src="/files/cp100a-google-cloud-platform-fundamentals/comparing-storage-options.png" /&gt;  &lt;/p&gt;
&lt;h2&gt;Lab 4&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://codelabs.developers.google.com/codelabs/cp100-cloud-storage/"&gt;Integrating Applications with Google Cloud Storage&lt;/a&gt;  &lt;/li&gt;
&lt;/ul&gt;
&lt;hr /&gt;
&lt;h1&gt;Module 5: Google Container Engine (GKE)&lt;/h1&gt;
&lt;h2&gt;What is a Container&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Virtualization at the operating system layer  &lt;/li&gt;
&lt;li&gt;Separates operating system from application code and dependencies  &lt;/li&gt;
&lt;li&gt;Isolates individual processes  &lt;/li&gt;
&lt;li&gt;Popular implementations include Docker and &lt;a href="https://coreos.com/rkt/docs/latest/"&gt;rkt&lt;/a&gt;  &lt;ul&gt;
&lt;li&gt;k8s 目前支援這兩種格式的 Container  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;OS =&amp;gt; Shared Libraries =&amp;gt; Contianer  &lt;ul&gt;
&lt;li&gt;安全性問題  &lt;ul&gt;
&lt;li&gt;會不會影響到別的 Container  &lt;/li&gt;
&lt;li&gt;把 kernel 弄爛了的話，別的 Container 也會一起爛掉。  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Why Use Container?&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Support consistency across development, testing, and production environments  &lt;/li&gt;
&lt;li&gt;Loose coupling between application and operating system layers  &lt;/li&gt;
&lt;li&gt;Much simpler to migrate workloads between on premises and cloud environments  &lt;/li&gt;
&lt;li&gt;Support agile development and operations  &lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a href="http://kubernetes.io/"&gt;Kubernetes (k8s)&lt;/a&gt;&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Open Source  &lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/kubernetes/kubernetes"&gt;https://github.com/kubernetes/kubernetes&lt;/a&gt;  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Google 的服務是跑在 &lt;a href="http://blog.kubernetes.io/2015/04/borg-predecessor-to-kubernetes.html"&gt;Borg&lt;/a&gt; 上面，Borg 是 k8s 的前身。  &lt;/li&gt;
&lt;li&gt;另外一個 Container 是 &lt;a href="https://dcos.io/"&gt;dcos&lt;/a&gt;  &lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/dcos/dcos"&gt;https://github.com/dcos/dcos&lt;/a&gt;  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Features of k8s&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Workload portability  &lt;ul&gt;
&lt;li&gt;Run in many environments, across cloud providers  &lt;/li&gt;
&lt;li&gt;Implementation is open and modular  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Rolling updates  &lt;ul&gt;
&lt;li&gt;Upgrade application with zero downtime  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Autoscaling  &lt;ul&gt;
&lt;li&gt;Automatically adapt to changes in workload  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Persistent storage  &lt;ul&gt;
&lt;li&gt;Abstracts details of how storage is provided from how it is consumed  &lt;/li&gt;
&lt;li&gt;有支援 MySQL Cluster  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Multi-zone clusters  &lt;ul&gt;
&lt;li&gt;Run a single cluster in multiple zones  &lt;/li&gt;
&lt;li&gt;Alpha on Google Cloud Platform  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Load balancing  &lt;ul&gt;
&lt;li&gt;External IP address routes traffic to correct port  &lt;/li&gt;
&lt;li&gt;Google 會幫你偵測機器的狀態，在機器死掉的時候幫你做 Migration  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Google Cloud Container Engine (GKE)&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Based on open source Kubernetes(k8s) orchestration system  &lt;/li&gt;
&lt;li&gt;Orchestrate and schedule Docker containers  &lt;/li&gt;
&lt;li&gt;Consumes Compute Engine instances and resources  &lt;/li&gt;
&lt;li&gt;Uses a declarative syntax to manage applications  &lt;ul&gt;
&lt;li&gt;JSON, YAML  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Decouple operational and development concerns  &lt;/li&gt;
&lt;li&gt;Manages and maintains  &lt;ul&gt;
&lt;li&gt;Logging  &lt;/li&gt;
&lt;li&gt;Health management  &lt;/li&gt;
&lt;li&gt;Monitoring  &lt;/li&gt;
&lt;li&gt;Scaling  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;補充  &lt;ul&gt;
&lt;li&gt;不只在 GCP 可以用，AWS 或是自己架都可以，因為是 Based on Open Source 的 k8s  &lt;/li&gt;
&lt;li&gt;可以執行很多 Container，彼此可以透過 k8s 達到 HA  &lt;/li&gt;
&lt;li&gt;目前的費用是算在 Compute Engine 上，因為實際還是開 GCE 然後在上面 run containers  &lt;/li&gt;
&lt;li&gt;目前以 GCE 的收費方式計價  &lt;/li&gt;
&lt;li&gt;Google Cloud Container Builder  &lt;ul&gt;
&lt;li&gt;Create Docker container images from app code in Google Cloud Storage  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Google Container Registry  &lt;ul&gt;
&lt;li&gt;Secure, private Docker image storage  &lt;blockquote&gt;
&lt;p&gt;沒記錯的話 images 是存在 Cloud Storage 上  &lt;/p&gt;
&lt;/blockquote&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="https://cloud.docker.com/"&gt;https://cloud.docker.com/&lt;/a&gt;  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Lab 5&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://codelabs.developers.google.com/codelabs/cp100-container-engine/#0"&gt;Deploying Applications Using Google Container Engine&lt;/a&gt;  &lt;/li&gt;
&lt;/ul&gt;
&lt;hr /&gt;
&lt;h1&gt;Module 6: Google Compute Engine and Networking&lt;/h1&gt;
&lt;h2&gt;Google Compute Engine&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Run large-csale workloads on virtual machines hosted on Google's infrastructure  &lt;/li&gt;
&lt;li&gt;Robust networking features  &lt;ul&gt;
&lt;li&gt;可以拿來做 MySQL cluster load balancer  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Instance metadata and startup scripts  &lt;ul&gt;
&lt;li&gt;每個 instance 會有 global 的 metadata 和各自的 metadata  &lt;/li&gt;
&lt;li&gt;startup script 也是放在 metadata 去做描述  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Persistent disk snapshots  &lt;/li&gt;
&lt;li&gt;High CPU, high memory, standard and shared-core machine types  &lt;/li&gt;
&lt;li&gt;HTTP and network load balancing  &lt;ul&gt;
&lt;li&gt;可以針對 Load Balancer 做個別的設定，會比 AWS 簡單。  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Advanced APIs for auto-scaling and group management  &lt;/li&gt;
&lt;li&gt;Innovative pricing  &lt;ul&gt;
&lt;li&gt;per &lt;em&gt;minute&lt;/em&gt; billing, sustained use discounts  &lt;/li&gt;
&lt;li&gt;Preemptible instances  &lt;/li&gt;
&lt;li&gt;High throughput to storage at no extra cost  &lt;/li&gt;
&lt;li&gt;Custom machine types - Only pay for the hardware you need  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;補充  &lt;ul&gt;
&lt;li&gt;Google 用 KVM 來實作這部份  &lt;/li&gt;
&lt;li&gt;可以在兩分多鐘內就開啟 1000 台機器  &lt;ul&gt;
&lt;li&gt;壓力測試跑了大概一個多小時，最後收到帳單大概是 500 美金左右。  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;硬碟必須至少要 200 GB 才會有一般的 performance, &amp;lt; 200 GB 的話會比較慢。  &lt;/li&gt;
&lt;li&gt;目前看到比較多的是拿來當 Load Balancer  &lt;/li&gt;
&lt;li&gt;目前 Load Balancer 使用 BSD 是會有問題的，因為缺少某些 Linux 才有的 Libraries。  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Google Cloud Networking&lt;/h2&gt;
&lt;h3&gt;Google Cloud Interconnect&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Carrier Interconnect  &lt;/li&gt;
&lt;li&gt;Direct Peering  &lt;ul&gt;
&lt;li&gt;需要有第 2 類電信執照才能申請  &lt;/li&gt;
&lt;li&gt;Connect your business directly to Google  &lt;/li&gt;
&lt;li&gt;所有流量的費用打對折，速度會更快，適合擁有 Data Center 的公司申請。  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Google Cloud VPN&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Secure connection over the Internet  &lt;/li&gt;
&lt;li&gt;Securely connect your network to Google Cloud Platform using IPsec VPN connection  &lt;/li&gt;
&lt;li&gt;Encrypts traffic over the Internet  &lt;/li&gt;
&lt;li&gt;Google Cloud Router supports dynamic routing between Google Cloud Platform and your network  &lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Google Cloud DNS&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Highly available and scalable DNS  &lt;/li&gt;
&lt;li&gt;Translates domain names into IP addresses  &lt;/li&gt;
&lt;li&gt;Create managed zones, then add, edit, delete DNS records  &lt;/li&gt;
&lt;li&gt;Programmatically manage zones and records using RESTful API or command- line interface  &lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Google Cloud Load Balancing&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;HTTP(s) load balancing  &lt;/li&gt;
&lt;li&gt;Balance HTTP-based traffic across multiple Compute Engine regions  &lt;/li&gt;
&lt;li&gt;Global, external IP address routes traffic  &lt;/li&gt;
&lt;li&gt;Scalable, requires no pre-warming and provides resilience, fault tolerance  &lt;/li&gt;
&lt;li&gt;TCP/SSL and UDP (network) load balancing  &lt;ul&gt;
&lt;li&gt;Spread TCP/SSL and UDP traffic over pool of instances within a Compute Engine region  &lt;/li&gt;
&lt;li&gt;Ensures only healthy instances handle traffic  &lt;/li&gt;
&lt;li&gt;Scalable, requires no pre-warming  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;補充  &lt;ul&gt;
&lt;li&gt;Global  &lt;ul&gt;
&lt;li&gt;可以在不同的 region 建 load balancer  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;HTTP(S) load balancing  &lt;/li&gt;
&lt;li&gt;Network load balancing  &lt;ul&gt;
&lt;li&gt;支援 Auto scaling  &lt;/li&gt;
&lt;li&gt;可以設定 protocol 跟 port  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;可以選擇 client IP + Protocol 的規則，看要導到哪台 Load Balancer  &lt;/li&gt;
&lt;li&gt;有隱藏 CDN 的功能，可以把 CDN 的功能打開。  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Operations and Tools&lt;/h2&gt;
&lt;h3&gt;Google Stackdriver&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Integrated monitoring, logging, diagnostics  &lt;/li&gt;
&lt;li&gt;Works across Google Cloud Platform, Amazon Web Services  &lt;/li&gt;
&lt;li&gt;Open source agents, integration  &lt;/li&gt;
&lt;li&gt;Powerful data, analytics tools  &lt;/li&gt;
&lt;li&gt;Collaborations with PagerDuty, BMC, Splunk, others  &lt;/li&gt;
&lt;li&gt;補充  &lt;ul&gt;
&lt;li&gt;可以針對條件去設定 alert  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;Cloud Monitoring&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;可以監控各種項目  &lt;/li&gt;
&lt;li&gt;可以自訂要監控哪些部份  &lt;/li&gt;
&lt;li&gt;可以和第三方應用程式銜接  &lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;Cloud Logging&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;可以幫你很輕鬆的檢視不同機器的 log  &lt;/li&gt;
&lt;li&gt;Log 線上保留三十天  &lt;/li&gt;
&lt;li&gt;支援 Export，讓你可以自己處理 Log  &lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Google Cloud Deployment Manager&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Infrastructure management service  &lt;/li&gt;
&lt;li&gt;Create a .yaml template describing your environment and use Deployment Manager to create resources  &lt;/li&gt;
&lt;li&gt;Provides repeatable deployments  &lt;/li&gt;
&lt;li&gt;補充  &lt;ul&gt;
&lt;li&gt;有點類似 Ansible 和 Chef  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Google Cloud Source Repositories&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Fully-featured Git repositories hosted on Google Cloud Platform  &lt;/li&gt;
&lt;li&gt;Supports collaborative development of cloud apps  &lt;/li&gt;
&lt;li&gt;Includes:  &lt;ul&gt;
&lt;li&gt;Source code editor  &lt;/li&gt;
&lt;li&gt;Integration with Stackdriver debugger  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Google Cloud Functions&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Create single-purpose functions that respond to events without a server or runtime  &lt;ul&gt;
&lt;li&gt;Event examples: New instance created, file added to Cloud Storage  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Written in Javascript, execute in managed Node.js environment on Google Cloud Platform  &lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Lab 6&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://codelabs.developers.google.com/codelabs/cp100-compute-engine/#0"&gt;Deploying Applications Using Google Compute Engine&lt;/a&gt;  &lt;/li&gt;
&lt;/ul&gt;
&lt;hr /&gt;
&lt;h1&gt;Module 7: Big Data and Machine Learning&lt;/h1&gt;
&lt;p&gt;&lt;img alt="Big Data Services" src="/files/cp100a-google-cloud-platform-fundamentals/big-data-services.png" /&gt;  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Fully managed, No-Ops Services  &lt;/li&gt;
&lt;li&gt;BigQuery  &lt;ul&gt;
&lt;li&gt;一個 column 就儲存一個 object，不是存 row。(column based)  &lt;ul&gt;
&lt;li&gt;不要下 &lt;code&gt;select *&lt;/code&gt;，會很慢，而且很貴，因為會對 process 的資料量收費。  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;每次 query 就透過 mapreduce 去做 macthing  &lt;/li&gt;
&lt;li&gt;可以透過 SQL-like 的語法(GQL)去查詢 big data  &lt;/li&gt;
&lt;li&gt;&lt;a href="https://drill.apache.org/"&gt;Apache drill&lt;/a&gt;  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Pub/Sub  &lt;ul&gt;
&lt;li&gt;建立一個 big data 用的 queue  &lt;/li&gt;
&lt;li&gt;比較常用的案例是 IoT  &lt;/li&gt;
&lt;li&gt;可搭配 dataflow 作 big data 的運算  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Dataflow  &lt;ul&gt;
&lt;li&gt;幫你整理資料  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Dataproc  &lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Big Data&lt;/h2&gt;
&lt;h3&gt;Google BigQuery&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Fully-managed analytics data warehouse  &lt;ul&gt;
&lt;li&gt;provides a service for near real-time interactive analysis of massive datasets (hundreds of TBs)  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Query using a SQL-like syntax (GQL)  &lt;/li&gt;
&lt;li&gt;Only pay for storage, processing used  &lt;/li&gt;
&lt;li&gt;Zero administration for performance and scale  &lt;/li&gt;
&lt;li&gt;Supports open standads  &lt;/li&gt;
&lt;li&gt;補充  &lt;ul&gt;
&lt;li&gt;當作 storage 和 analyze 的工具  &lt;/li&gt;
&lt;li&gt;類似 &lt;a href="https://cassandra.apache.org/"&gt;Cassandra&lt;/a&gt;  &lt;/li&gt;
&lt;li&gt;Column-based  &lt;/li&gt;
&lt;li&gt;1 TB 的資料大概花 6 秒就可以 scan 完  &lt;/li&gt;
&lt;li&gt;一次會幫你開很多機器去做運算，最後吐回一個結果給你  &lt;/li&gt;
&lt;li&gt;切忌用 &lt;code&gt;select *&lt;/code&gt;  &lt;/li&gt;
&lt;li&gt;有 dry run 可以先告訴你這個 Query 下下去會花多少錢  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Google Cloud Pub/Sub&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Scalable and reliable messaging for Google Cloud Platform and beyond  &lt;/li&gt;
&lt;li&gt;Supports many-to-many asynchronous messaging  &lt;/li&gt;
&lt;li&gt;Includes support for offline consumers  &lt;/li&gt;
&lt;li&gt;Based on proven Google technologies  &lt;/li&gt;
&lt;li&gt;Integrates with Cloud Dataflow for data processing pipelines  &lt;/li&gt;
&lt;li&gt;Uses push/pull subscriptions to topics  &lt;/li&gt;
&lt;li&gt;Use cases:  &lt;ul&gt;
&lt;li&gt;Building block for data ingestion in Dataflow, Internet of Things (IoT), Marketing Analytics  &lt;/li&gt;
&lt;li&gt;Foundation for Dataflow streaming  &lt;/li&gt;
&lt;li&gt;Push notifications for cloud-based applications  &lt;/li&gt;
&lt;li&gt;Connect applications across Google Cloud Platform (push/pull between Compute Engine and App Engine)  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Google Cloud Dataflow&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Managed service for executing scalable and reliable data pipelines  &lt;/li&gt;
&lt;li&gt;Write code once and get batch and streaming  &lt;ul&gt;
&lt;li&gt;Transform-based programming model  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Clusters are sized for you  &lt;/li&gt;
&lt;li&gt;Processes data using Compute Engine instances  &lt;/li&gt;
&lt;li&gt;Integrates with GCP services like Cloud  Storage, Cloud Pub/Sub, BigQuery, Bigtable  &lt;/li&gt;
&lt;li&gt;Open source Java and Python SDKs  &lt;/li&gt;
&lt;li&gt;Use cases:  &lt;ul&gt;
&lt;li&gt;ETL (extract/transform/load) pipelines to move, filter, enrich, shape data  &lt;/li&gt;
&lt;li&gt;Data analysis - batch computation or continuous computation using streaming  &lt;/li&gt;
&lt;li&gt;Orchestration - create pipelines that coordinate services, including external services  &lt;ul&gt;
&lt;li&gt;可以很容易的和其他服務整合  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Google Cloud Dataproc&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Fast, easy, managed way to run Hadoop and Spark/Hive/Pig on Google Cloud Platform  &lt;/li&gt;
&lt;li&gt;Benefit from cloud integration  &lt;ul&gt;
&lt;li&gt;Cloud Storage  &lt;/li&gt;
&lt;li&gt;Stackdriver  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Customize and configure clusters using initialization actions  &lt;/li&gt;
&lt;li&gt;Create clusters in 90 sec or less  &lt;/li&gt;
&lt;li&gt;Dataproc clusters billed minute-by-minute  &lt;ul&gt;
&lt;li&gt;Save money using preemptible instances for batch processing  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Scale clusters up and down even when jobs are running  &lt;/li&gt;
&lt;li&gt;Developer tools  &lt;ul&gt;
&lt;li&gt;RESTful API  &lt;/li&gt;
&lt;li&gt;Integration with Google Cloud SDK  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Use cases:  &lt;ul&gt;
&lt;li&gt;Easily migrate on-premises Hadoop jobs to the cloud  &lt;/li&gt;
&lt;li&gt;Quickly analyze data (like log data) stored in Cloud Storage - create a cluster in less than 2 minutes then delete it immediately  &lt;/li&gt;
&lt;li&gt;Use Spark/Spark SQL to quickly to perform data mining and analysis  &lt;ul&gt;
&lt;li&gt;Spark SQL 可以讓你比較好操控資料  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Use Spark Machine Learning Libraries (MLlib) to run classification algorithms  &lt;ul&gt;
&lt;li&gt;Spark 最強的部份就是 MLlib，但之後可能會被 Google 推出的 TensorFlow API 取代掉也不一定  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;補充  &lt;ul&gt;
&lt;li&gt;Cluster  &lt;/li&gt;
&lt;li&gt;HDFS work node  &lt;/li&gt;
&lt;li&gt;完整的 Hadoop 類型服務  &lt;/li&gt;
&lt;li&gt;可以在 WebUI 上面選擇 node 數目  &lt;/li&gt;
&lt;li&gt;要自己寫 mapreduce  &lt;/li&gt;
&lt;li&gt;支援直接撈 Cloud Storage 的資料，甚至可以把資料送到 BigQuery  &lt;/li&gt;
&lt;li&gt;create cluster 後要 submit job，只要寫好 mapreduce 和 jar 檔，就可以直接幫你處理資料  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a href="https://datalab.cloud.google.com/"&gt;Google Cloud Datalab&lt;/a&gt;&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Interactive tool for large-scale data exploration, transformation, analysis, visualization  &lt;/li&gt;
&lt;li&gt;Analyze data in BigQuery, Compute Engine, and Cloud Storage using Python, SQL, and JavaScript  &lt;/li&gt;
&lt;li&gt;Easily deploy transformation, analysis models to BigQuery  &lt;/li&gt;
&lt;li&gt;Integrated, open source  &lt;ul&gt;
&lt;li&gt;Runs on Google App Engine  &lt;/li&gt;
&lt;li&gt;Built on Jupyter (formerly IPython)  &lt;/li&gt;
&lt;li&gt;Use Google Charts or matplotlib for easy visualizations  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Code, documentation, results, visualizations in intuitive notebook format  &lt;/li&gt;
&lt;li&gt;補充  &lt;ul&gt;
&lt;li&gt;可以透過 Google 去銜接很多 Datasource，可以做整合，例如匯出報表。  &lt;/li&gt;
&lt;li&gt;有支援 BigQuery, Cloud Dataflow，可以利用他們去做分析  &lt;/li&gt;
&lt;li&gt;用法跟 Jupyter Notebook 差不多  &lt;/li&gt;
&lt;li&gt;是使用 Managed VM 來用 Datalab，該 VM 會裝一些套件，然後透過 GAE 去操作。  &lt;ul&gt;
&lt;li&gt;安裝好後會變成 GAE 裡頭的其中一個 service  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Machine Learning (Google Cloud ML)&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://cloud.google.com/vision/"&gt;Vision API&lt;/a&gt;  &lt;/li&gt;
&lt;li&gt;&lt;a href="https://cloud.google.com/speech/"&gt;Speech API&lt;/a&gt;  &lt;/li&gt;
&lt;li&gt;&lt;a href="https://cloud.google.com/translate/docs/"&gt;Translate API&lt;/a&gt;  &lt;/li&gt;
&lt;li&gt;&lt;a href="https://cloud.google.com/prediction/"&gt;Prediction API&lt;/a&gt;  &lt;/li&gt;
&lt;li&gt;Google Cloud Machine Learning Use Cases  &lt;ul&gt;
&lt;li&gt;Structured Data  &lt;ul&gt;
&lt;li&gt;Classification / Regression  &lt;ul&gt;
&lt;li&gt;Customer churn analysis  &lt;/li&gt;
&lt;li&gt;Product diagnostics  &lt;/li&gt;
&lt;li&gt;Forecasting  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Recommendation  &lt;ul&gt;
&lt;li&gt;Content personalization  &lt;/li&gt;
&lt;li&gt;Product X-sells/up-sells  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Anomaly Detection  &lt;ul&gt;
&lt;li&gt;Fraud detection  &lt;/li&gt;
&lt;li&gt;Asset sensor diagnostics  &lt;/li&gt;
&lt;li&gt;Log metric anomalies  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Unstructured Data  &lt;ul&gt;
&lt;li&gt;Image Analytics  &lt;ul&gt;
&lt;li&gt;Identify damaged shipments  &lt;/li&gt;
&lt;li&gt;Explicit content classification  &lt;/li&gt;
&lt;li&gt;Identify “styles” in images  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Text Analytics  &lt;ul&gt;
&lt;li&gt;Call center log analysis  &lt;/li&gt;
&lt;li&gt;Language identification  &lt;/li&gt;
&lt;li&gt;Topic classification  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Sentiment analysis  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Lab 7&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://codelabs.developers.google.com/codelabs/cp100-big-query/#0"&gt;Getting Started with BigQuery&lt;/a&gt;  &lt;/li&gt;
&lt;/ul&gt;
&lt;hr /&gt;
&lt;h1&gt;Questions&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;一個帳號可以管理的 Project 上限是多少？  &lt;/li&gt;
&lt;li&gt;GAE serving static 不用開 instance?  &lt;/li&gt;
&lt;li&gt;Project migration 的建議  &lt;/li&gt;
&lt;li&gt;Bigtable 和 BigQuery 的主要差異  &lt;/li&gt;
&lt;li&gt;GKE 的 MySQL cluster  &lt;/li&gt;
&lt;li&gt;GAE 的 Front-end instances 跟 Back-end instances 的差別  &lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;其實還有很多問題啦，只是沒有太多時間可以問，&lt;br /&gt;
而且要在網路上發問又必須描述的很詳細，&lt;br /&gt;
然後 Facebook 又是個黑洞，很難找之前的發文內容，&lt;br /&gt;
實在不太喜歡拿 Facebook 來問問題。&lt;br /&gt;
所以可能就自己 Google 、親自實驗或之後有機會再在 GCPUG.tw 當面問吧  &lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr /&gt;
&lt;h1&gt;相關連結&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.facebook.com/groups/GCPUG.TW/"&gt;https://www.facebook.com/groups/GCPUG.TW/&lt;/a&gt;  &lt;ul&gt;
&lt;li&gt;臺灣的 Google Cloud Platform User Group，有問題歡迎在上面發問討論。  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="https://cloud.google.com/products/calculator/"&gt;Google Cloud Platform Pricing Calculator | Google Cloud Platform&lt;/a&gt;  &lt;ul&gt;
&lt;li&gt;GCP Pricing 試算  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/GoogleCloudPlatformTraining"&gt;https://github.com/GoogleCloudPlatformTraining&lt;/a&gt;  &lt;ul&gt;
&lt;li&gt;GCP 教材的 Lab code  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;hr /&gt;
&lt;blockquote&gt;
&lt;p&gt;有種吃了 GCP 大還丹的感覺，需要時間消化。&lt;br /&gt;
能夠在上班時間來 Google Taipei 上課實在太棒了！&lt;br /&gt;
謝謝同事 Finley 一直被我煩被我問問題 XD&lt;br /&gt;
感謝老闆 Teddy，也感謝辛苦的講師 Simon。  &lt;/p&gt;
&lt;/blockquote&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">m157q</dc:creator><pubDate>Wed, 25 May 2016 22:47:00 +0800</pubDate><guid isPermaLink="false">tag:blog.m157q.tw,2016-05-25:posts/2016/05/25/cp100a-google-cloud-platform-fundamentals/</guid><category>Google Cloud Platform</category></item></channel></rss>